#pragma once
#include <cassert>
#include <cmath>
#include <csignal>
#include <cstdlib>
#include <fcntl.h>
#include <memory>
#include <random>
#include <ranges>
#include <utility>
#include <vector>
#include <optional>
#include <cstdint>
#include <string>
#include <fstream>
#include <print>
#include <atomic>
#include <thread>
#include <format>

// single precision for now, ~15% faster than double, but double precision is an option for maximum accuracy
using float_T = float;

constexpr float_T epsilon = 1e-6;
constexpr float_T PI = M_PI;

inline bool implies(bool a, bool b){
    return !a || b;
}

/// generate a uniformly distributed random float in [0, 1)
inline float_T randomFloat() {
    // initially generated by ai, thread safe by me
    // make thread safe by ensuring each thread has its own generator
    thread_local static std::random_device rd;
    thread_local static std::mt19937 gen(rd());
    thread_local static std::uniform_real_distribution<float_T> dis(0., 1.);
    return dis(gen);
}

struct Vec2{
    float_T x,y;

    // see Vec3 for details on these operators

    Vec2 operator+(const Vec2& other) const{
        return Vec2(x+other.x, y+other.y);
    }

    Vec2 operator-(const Vec2& other) const{
        return Vec2(x-other.x, y-other.y);
    }

    Vec2 operator*(float_T scalar) const{
        return Vec2(x*scalar, y*scalar);
    }

    Vec2 operator/(float_T scalar) const{
        return Vec2(x/scalar, y/scalar);
    }

    Vec2 operator*(const Vec2& other) const{
        return Vec2(x*other.x, y*other.y);
    }

    Vec2 operator/(const Vec2& other) const{
        return Vec2(x/other.x, y/other.y);
    }

    friend Vec2 operator*(float_T scalar, const Vec2& vec) {
        return vec * scalar;
    }

    float_T dot(const Vec2& other) const{
        return x*other.x + y*other.y;
    }

    float_T length() const{
        return std::sqrt(dot(*this));
    }

    Vec2 normalized() const{
        return *this / length();
    }
};

struct Vec3{
    float_T x,y,z;

    constexpr Vec3(float_T x, float_T y, float_T z) : x(x), y(y), z(z){ }

    constexpr explicit Vec3(float_T uniform) : x(uniform), y(uniform), z(uniform){ }

    constexpr Vec3() : Vec3(0.) {}

    Vec3 operator+(const Vec3& other) const{
        return Vec3(x+other.x, y+other.y, z+other.z);
    }

    Vec3 operator-(const Vec3& other) const{
        return Vec3(x-other.x, y-other.y, z-other.z);
    }

    // scalar multiplication/division
    Vec3 operator*(float_T scalar) const{
        return Vec3(x*scalar, y*scalar, z*scalar);
    }

    // Friend function to overload for scalar * vector
    friend Vec3 operator*(float_T scalar, const Vec3& vec) {
        return vec * scalar;
    }
    friend Vec3 operator/(float_T scalar, const Vec3& vec) {
        return vec / scalar;
    }

    Vec3 operator/(float_T scalar) const{
        return Vec3(x/scalar, y/scalar, z/scalar);
    }

    // elementwise multiplication/division
    Vec3 operator*(Vec3 other) const{
        return Vec3(x*other.x, y*other.y, z*other.z);
    }

    Vec3 operator/(Vec3 other) const{
        return Vec3(x/other.x, y/other.y, z/other.z);
    }

    bool operator==(const Vec3& other) const{
        // epsilon comparison
        return std::abs(x - other.x) < epsilon &&
            std::abs(y - other.y) < epsilon &&
            std::abs(z - other.z) < epsilon;
    }

    float_T operator[](size_t index) const{
        assert(index < 3 && "Index out of bounds");
        return index == 0 ? x : (index == 1 ? y : z);
    }

    Vec3 operator-() const {
        return *this * -1;
    }

    // state modifying operators
    Vec3& operator+=(const Vec3& other){
        x += other.x;
        y += other.y;
        z += other.z;
        return *this;
    }
    Vec3& operator*=(float_T scalar){
        x *= scalar;
        y *= scalar;
        z *= scalar;
        return *this;
    }
    Vec3& operator/=(float_T scalar){
        x /= scalar;
        y /= scalar;
        z /= scalar;
        return *this;
    }

    float_T dot(const Vec3& other) const{
        return x*other.x + y*other.y + z*other.z;
    }

    float_T length() const{
        return std::sqrt(dot(*this));
    }

    Vec3 normalized() const{
        return *this / length();
    }

    Vec3 cross(const Vec3& other) const {
        return Vec3(
                 y*other.z - z*other.y,
                 z*other.x - x*other.z,
                 x*other.y - y*other.x
               );
    }

    Vec3 clamp(float_T min, float_T max){
        return Vec3(
            std::clamp(x, min, max),
            std::clamp(y, min, max),
            std::clamp(z, min, max)
        );
    }

    Vec3 lerp(const Vec3& other, float_T t) const{
        return *this * (1-t) + other * t;
    }

    Vec3 min(const Vec3& other) const{
        return Vec3(
            std::min(x, other.x),
            std::min(y, other.y),
            std::min(z, other.z)
        );
    }

    Vec3 max(const Vec3& other) const{
        return Vec3(
            std::max(x, other.x),
            std::max(y, other.y),
            std::max(z, other.z)
        );
    }

    float_T distance(const Vec3& other) const{
        return (*this - other).length();
    }

    Vec3 reflect(const Vec3& normal) const{
        const Vec3 incomingDirection = *this;
        return incomingDirection - normal * 2 * (incomingDirection.dot(normal));
    }

    // === static helpers ===

    static std::pair</* tangent */ Vec3, /* bitangent */ Vec3> createOrthonormalBasis(const Vec3& N) {
        assert(N == N.normalized() && "N must be a normal vector");

        // First, pick a helper vector that's not parallel to N
        Vec3 helper = std::abs(N.y) < 0.999f ? Vec3(0, 1, 0) : Vec3(1, 0, 0);

        // Construct X (tangent/T) to be perpendicular to N using the helper
        Vec3 X = N.cross(helper).normalized();

        // Construct Y (bitangent/B) to be perpendicular to both N and X
        Vec3 Y = N.cross(X);  // NOTE: no need to normalize since N and X are unit vectors
                                     // and perpendicular to each other

        assert(X == X.normalized() && "X must be normalized");
        assert(Y == Y.normalized() && "Y must be normalized");

        // For debugging, these assertions should now pass
        assert(std::abs(X.dot(Y)) < epsilon && "X and Y must be orthogonal");
        assert(std::abs(N.dot(Y)) < epsilon && "N and Y must be orthogonal");
        assert(std::abs(N.dot(X)) < epsilon && "N and X must be orthogonal");

        return {X, Y};
    }

};

// overload std::format/std::println for Vec3
template <>
struct std::formatter<Vec3> : std::formatter<std::string> {
  auto format(const Vec3& v, format_context& ctx) const {
    return formatter<string>::format(std::format("[{}, {}, {}]", v.x, v.y, v.z), ctx);
  }
};

/// fresnel effect with Schlick's approximation
inline float_T schlickFresnel(float_T intensity, float_T LdotH){
    return intensity + (1. - intensity) * std::pow(1.0f - LdotH, 5);
}
inline Vec3 schlickFresnel(Vec3 color, float_T LdotH){
    return Vec3(schlickFresnel(color.x, LdotH), schlickFresnel(color.y, LdotH), schlickFresnel(color.z, LdotH));
}

struct PPMWriter{
private:
    std::ofstream file;

    std::ofstream::pos_type pixelDataStart;


public:
    std::string filePath;
    uint32_t width, height;

    PPMWriter(std::string_view filePath, uint32_t width, uint32_t height)
        : filePath(filePath), width(width), height(height){
        file = std::ofstream(this->filePath, std::ios::binary);
        if(file.fail()){
            std::perror("Couldn't open file for writing");
            std::exit(EXIT_FAILURE);
        }

        // we're writing binary ppm, i.e. P6

        // write header
        file << "P6\n" << width << " " << height << "\n255\n";

        // the rest is the pixel data, which we'll write later
        this->pixelDataStart = file.tellp();
    }

    /// write a single pixel in binary format, pixels are iterated over row by row
    void writePixel(uint8_t r, uint8_t g, uint8_t b){
        file.put(r);
        file.put(g);
        file.put(b);
    }

    void writePixel(Vec3 color){
        assert(color == color.clamp(0,1) && "Color must be in the range [0,1]");
        writePixel(
            static_cast<uint8_t>(color.x * 255),
            static_cast<uint8_t>(color.y * 255),
            static_cast<uint8_t>(color.z * 255)
        );
    }

    void rewind(){
        file.seekp(pixelDataStart);
    }

    void flush(){
        file.flush();
    }
};


/// A ray, represented by its origin and direction
struct Ray{
    Vec3 origin;
    Vec3 direction;
    Vec3 invDirection;

    // constructors are not allowed for readability, createXyz functions should be used instead
private:
    /// assumes that the direction is normalized!
    Ray(Vec3 origin, Vec3 direction, Vec3 invDirection)
        : origin(origin), direction(direction), invDirection(invDirection){
        assert(direction == direction.normalized() && "Ray direction must be normalized");
        assert(invDirection == 1./direction && "Ray inverse direction must be the reciprocal of the direction");
    }

    Ray(Vec3 origin, Vec3 direction) : Ray(origin, direction, 1./direction){}

public:

    /// assumes that the direction is normalized!
    /// creates a ray exactly as specified
    static Ray createExact(Vec3 origin, Vec3 direction){
        return Ray(origin, direction);
    }

    /// Slightly offsets the ray from its origin in the direction to avoid self intersections
    /// with the object it might have originated from
    /// assumes that the direction is normalized!
    /// - epsilonFactor: how much the ray is offset from the origin in terms of the epsilon - the default of 10 is ususally fine
    static Ray createWithOffset(Vec3 origin, Vec3 direction, float_T epsilonFactor = 10.){
        return Ray(origin + direction * epsilonFactor * epsilon, direction);
    }
};

struct Camera{
    Vec3 position;
    Vec3 direction;
    Vec3 down; // PPM has (0,0) in the top left, so we want to go down not up
    Vec3 right;
    float_T width;
    uint64_t widthPixels;
    float_T height;
    uint64_t heightPixels;
    float_T exposure;

    // to make the images at the provided exposure look more like the reference
    static constexpr float_T exposureCorrectionFactor = 15.;

    virtual ~Camera() = default; 

    // TODO maybe experiment with && and std::move to avoid some copies
    Camera(Vec3 position,
           Vec3 lookAt,
           Vec3 up,
           float_T width,
           float_T height,
           float_T exposure) : position(position), direction((lookAt - position).normalized()), down(-(up.normalized())), right(direction.cross(down).normalized()), width(width), widthPixels(std::round(width)), height(height), heightPixels(std::round(height)), exposure(exposure){
        const float_T aspectRatio = width / height;
        imagePlaneDimensions = Vec2(aspectRatio*imagePlaneHeight, imagePlaneHeight);
    }

    /// gets a pixel in pixel screen space, i.e. [0,width]x[0,height]
    /// outputs a ray in world space, i.e. adjusting for the camera's position
    virtual Ray generateRay(Vec2 pixelInScreenSpace) const = 0;

protected:
    float_T imagePlaneHeight = 1.0;
    Vec2 imagePlaneDimensions;

    /// gets a pixel in pixel screen space, i.e. [0,width]x[0,height]
    Vec3 pixelInWorldSpace(Vec2 pixelInScreenSpace) const {
        // so, the pixel is in the range [0,width]x[0,height]
        // we want to map this to [-0.5,0.5]x[-0.5,0.5] in the camera's space
        // which is then mapped to the image plane in world space

        constexpr float_T pixelWidthHeight = 1.0;

        Vec2 pixelCenterInCameraSpace = Vec2(
            (pixelInScreenSpace.x + /* use center of pixel */ pixelWidthHeight/2) / width - 0.5,
            (pixelInScreenSpace.y + pixelWidthHeight/2) / height - 0.5
        );

        // scale to world space scale (no translation yet) by multiplying by the image plane dimensions
        Vec2 pixelScaledByWorldSpace = Vec2(pixelCenterInCameraSpace.x * imagePlaneDimensions.x, pixelCenterInCameraSpace.y * imagePlaneDimensions.y);

        // now after scaling to world space, translate to world space (i.e. the cameras position), and add the camera's right/down directions
        Vec3 pixelOrigin = position + right*pixelScaledByWorldSpace.x + down*pixelScaledByWorldSpace.y;
        return pixelOrigin;
    }

    void setImagePlaneDimensionsFromFOV(float_T fovDegrees){
        // Convert FOV to radians
        const float_T verticalFOVRad = fovDegrees * (PI / 180.0);
        imagePlaneHeight = 2. * tan(verticalFOVRad / 2.); // Distance to image plane is 1 unit
        // calculate image plane width from pixel screen aspect ratio
        const float_T aspectRatio = width / height;
        imagePlaneDimensions = Vec2(imagePlaneHeight * aspectRatio, imagePlaneHeight);
    }
};

struct OrthographicCamera : public Camera{

    OrthographicCamera(Vec3 position, Vec3 lookAt, Vec3 up, float_T width, float_T height, float_T exposure)
        : Camera(position, lookAt, up, width, height, exposure){ }

    virtual Ray generateRay(Vec2 pixelInScreenSpace) const override{
        // for an orthographic camera, basically just shoot a ray in the look direction, through the pixel center
        return Ray::createExact(
            pixelInWorldSpace(pixelInScreenSpace),
            direction
        );
    }
};

struct PinholePerspectiveCamera : public Camera{
    PinholePerspectiveCamera(
        Vec3 position,
        Vec3 lookAt,
        Vec3 up,
        float_T fovDegrees,
        float_T width,
        float_T height,
        float_T exposure)
        : Camera(position, lookAt, up, width, height, exposure) {
        // Calculate image plane height based on FOV and set image plane dimensions
        setImagePlaneDimensionsFromFOV(fovDegrees);
    }

    /// gets a pixel in pixel screen space, i.e. [0,width]x[0,height]
    /// outputs a ray in world space, i.e. adjusting for the camera's position
    /// Generates a ray from the camera position through the specified pixel
    virtual Ray generateRay(Vec2 pixelInScreenSpace) const override {
        // Use pixelInWorldSpace to get the point on the image plane in world space
        Vec3 pointOnImagePlane = pixelInWorldSpace(pixelInScreenSpace) + /* place image plane 1 unit away from camera */ direction;

        // Calculate ray direction from camera position to point on image plane
        Vec3 rayDirection = (pointOnImagePlane - position).normalized();

        return Ray::createExact(position, rayDirection);
    }
};

/// thin lens camera with depth of field
/// aperture does not affect the amound of light let in, only the depth of field,
/// as its easier to adjust the amount of light via the exposure
struct SimplifiedThinLensCamera : public Camera {
    float_T focalLength;
    float_T apertureRadius;
    // Distance to focal plane
    float_T focusDistance; 

    /// takes focal length in mm, focal distance in meters
    SimplifiedThinLensCamera(
        Vec3 position,
        Vec3 direction,
        Vec3 up,
        float_T fovDegrees,
        float_T width,
        float_T height,
        float_T exposure,
        float_T fStop,
        float_T focalLength,       // in mm
        float_T focusDistance)     // in meters
        : Camera(position, direction, up, width, height, exposure),
          focalLength(0.001 * focalLength), // convert mm to meter
          apertureRadius((0.001 * focalLength / fStop) /* = diameter, so halve it to get radius */ * 0.5),
          focusDistance(focusDistance) {
        // TODO for a proper (i.e. not simplified) thin lens camera, should use the focal length to calculate the image plane dimensions, and place it behind the lens
        setImagePlaneDimensionsFromFOV(fovDegrees);
    }

    virtual Ray generateRay(Vec2 pixelInScreenSpace) const override {
        // generated by ai

        // Get the point on the image plane in world space
        // TODO for a proper (i.e. not simplified) thin lens camera, should use the focal length as distance to image plane, not just 1 (and place image plane/sensor behind the lens)
        Vec3 pointOnImagePlane = pixelInWorldSpace(pixelInScreenSpace) + direction;

        // Calculate the direction from the camera position to the point on the image plane
        Vec3 rayDirection = (pointOnImagePlane - position).normalized();

        // Sample a point on the lens aperture
        Vec2 lensSample = sampleAperture();
        Vec3 lensPoint = position + right * lensSample.x + down * lensSample.y;

        // Calculate the point on the focal plane
        float_T t = focusDistance / rayDirection.dot(direction);
        Vec3 focalPoint = position + rayDirection * t;

        // Calculate the new ray direction from the lens point to the focal point
        Vec3 newRayDirection = (focalPoint - lensPoint).normalized();

        return Ray::createExact(lensPoint, newRayDirection);
    }

private:
    Vec2 sampleAperture() const {
        float_T theta = 2.0 * PI * randomFloat();
        float_T r = sqrt(randomFloat()) * apertureRadius;
        // Convert from local space to world space using image plane dimensions per pixel
        // for the purposes of the aperture, we need to assume the image plane has its "correct" dimensions, i.e. scale it down by the focal length
        // TODO explain 0.001 factor
        return Vec2(r * cos(theta), r * sin(theta)) * 1000. * focalLength;
    }

};

struct Color8Bit{
    uint8_t r,g,b;
};

struct Texture{
private:
    uint32_t width, height;
    std::vector<Color8Bit> pixels;

public:
    Texture(uint32_t width, uint32_t height, std::vector<Color8Bit> pixels)
        : width(width), height(height), pixels(pixels){
        assert(width * height == pixels.size() && "Texture dimensions don't match pixel count");
    }

    Vec3 colorAt(const Vec2& textureCoords) const{
        // wrap around
        float_T x = std::fmod(textureCoords.x, 1.);
        float_T y = std::fmod(textureCoords.y, 1.);

        // scale to pixel space
        uint32_t pixelX = static_cast<uint32_t>(x * width);
        uint32_t pixelY = static_cast<uint32_t>(y * height);

        auto pixel = pixels[pixelY * width + pixelX];
        return Vec3(pixel.r / 255., pixel.g / 255., pixel.b / 255.);
    }
};

struct MaterialBase{
    Vec3 diffuseBaseColor;
    // share textures to reduce memory usage
    std::optional<std::shared_ptr<Texture>> texture;

    /// if the material has a texture, get the diffuse color at the given texture coordinates,
    /// otherwise just return the diffuse color
    /// The texture coordinates here need to be interpolated from the vertices of e.g. the triangle
    Vec3 diffuseColorAtTextureCoords(const Vec2& textureCoords) const {
        if(texture.has_value())
            return (*texture)->colorAt(textureCoords);
        else
            return diffuseBaseColor;
    }
};

struct PhongMaterial : MaterialBase {
    Vec3 specularColor;
    float_T ks,kd;
    uint64_t specularExponent;
    std::optional<float_T> reflectivity;
    std::optional<float_T> refractiveIndex;

    static constexpr float_T ambientIntensity = 0.25;

    constexpr PhongMaterial(
            Vec3 diffuseColor,
            Vec3 specularColor,
            float_T ks,
            float_T kd,
            uint64_t specularExponent,
            std::optional<float_T> reflectivity,
            std::optional<float_T> refractiveIndex,
            std::optional<std::shared_ptr<Texture>> texture)
        : MaterialBase(diffuseColor, texture), specularColor(specularColor), ks(ks), kd(kd), specularExponent(specularExponent), reflectivity(reflectivity), refractiveIndex(refractiveIndex) { }

    /// if the material has a texture, get the diffuse color at the given texture coordinates,
    /// otherwise just return the diffuse color
    /// The texture coordinates here need to be interpolated from the vertices of e.g. the triangle
    Vec3 diffuseColorAtTextureCoords(const Vec2& textureCoords) const {
        if(texture.has_value())
            return (*texture)->colorAt(textureCoords);
        else
            return diffuseBaseColor;
    }

};

/// Disney-like Principled BRDF
struct PrincipledBRDFMaterial : MaterialBase{
    float_T emissiveness;

    float_T baseColorDiffuseIntensity;
    float_T metallic;
    float_T subsurface;
    float_T specular;
    float_T roughness;
    float_T specularTint;
    float_T anisotropic;
    float_T sheen;
    float_T sheenTint;
    float_T clearcoat;
    float_T clearcoatGloss;

    // TODO explain V, L, H, N, X, Y terminology

    constexpr PrincipledBRDFMaterial(
            Vec3 diffuseColor,
            std::optional<std::shared_ptr<Texture>> texture,
            float_T emissiveness,
            float_T baseColorIntensity,
            float_T metallic,
            float_T subsurface,
            float_T specular,
            float_T roughness,
            float_T specularTint,
            float_T anisotropic,
            float_T sheen,
            float_T sheenTint,
            float_T clearcoat,
            float_T clearcoatGloss)
        : MaterialBase(diffuseColor, texture), emissiveness(emissiveness), baseColorDiffuseIntensity(baseColorIntensity), metallic(metallic), subsurface(subsurface), specular(specular), roughness(roughness), specularTint(specularTint), anisotropic(anisotropic), sheen(sheen), sheenTint(sheenTint), clearcoat(clearcoat), clearcoatGloss(clearcoatGloss) {

            // Diffuse weight only depends on:
            // - metallic (metals don't have diffuse)
            // - clearcoat (reduces underlying diffuse)
            // - and the diffuse intensity itself
            diffuseWeight = (1.0f - metallic) * baseColorDiffuseIntensity * (1.0f - 0.5f * clearcoat * clearcoatGloss);

            // Specular weight depends on:
            // - metallic (increases specular)
            // - roughness (decreases specular more aggressively)
            // - specular parameter
            specularWeight = (1.0f + metallic) * specular * (1.0f - roughness * roughness);

            clearcoatWeight = clearcoat * clearcoatGloss;

            // normalize them
            float_T total = diffuseWeight + specularWeight + clearcoatWeight;
            diffuseWeight   /= total;
            specularWeight  /= total;
            clearcoatWeight /= total;
        }

    Vec3 emissionColor(const Vec2& textureCoords) const {
        return emissiveness * diffuseColorAtTextureCoords(textureCoords);
    }

    // NOTE: the implementation is based off of generative AI, heavily refined by hand

private:
    // sampling weights for BRDF sampling components
    // these weights always add up to 1

    float_T diffuseWeight;
    float_T specularWeight;
    float_T clearcoatWeight;

    // this seems to be the standard value from the literature
    static constexpr float_T fixedClearcoatRoughness = 0.25;

    // === Utility functions ===

    // maths
    static float_T sqr(float_T x) { return x * x; }
    static float_T safe_sqrt(float_T x) { return std::sqrt(std::max(epsilon, x)); }

    // optics
    // (also see schlickFresnel)
    static float_T luminance(const Vec3& color) {
        return std::max(epsilon, color.dot(Vec3(0.3f, 0.6f, 0.1f)));
    }

    // Distribution functions
    static float_T GTR1(float_T NdotH, float_T a) {
        if (a >= 1.0f) return (float_T)(1.0 / PI);
        float_T a2 = std::max(epsilon, a * a);
        float_T t = 1.0 + (a2 - 1.0) * NdotH * NdotH;
        return (a2 - 1.0f) / (PI * std::log(a2) * t);
    }

    // GGX (Trowbridge-Reitz) Distribution Function
    static float_T D_GGX_aniso(const Vec3& H, const Vec3& N,
                              const Vec3& X, const Vec3& Y,
                              float_T ax, float_T ay) {
        float_T NdotH = std::max(epsilon, N.dot(H));

        // Early exit if normal and half vector are perpendicular
        if (NdotH <= 0) return 0;

        // Project H onto the tangent plane
        float_T HdotX = H.dot(X);
        float_T HdotY = H.dot(Y);

        // Calculate the squared slopes
        float_T ax2 = sqr(ax);
        float_T ay2 = sqr(ay);

        // Calculate the normalization factor
        float_T denom = (sqr(HdotX) / ax2 + sqr(HdotY) / ay2 + sqr(NdotH));

        return 1.0f / (PI * ax * ay * sqr(denom));
    }

    /// Calculate anisotropic roughness parameters
    static std::pair<float_T, float_T> calculateAnisotropicParams(float_T roughness, float_T anisotropic) {
        // TODO maybe get rid of parameters and make non-static?
        roughness = std::max(0.001f, roughness);
        
        // Modify how anisotropic affects the aspect ratio
        // Map anisotropic [0,1] to a more useful range for aspect ratio
        float_T t = anisotropic * 0.9f;  // Keep maximum anisotropy slightly below 1
        float_T ax = std::max(0.001f, roughness * (1.0f + t));
        float_T ay = std::max(0.001f, roughness * (1.0f - t));
        return {ax, ay};
    }

    Vec3 sampleGGX(float_T roughness, const Vec3& X, const Vec3& Y, const Vec3& N) const {
        float_T u1 = randomFloat();
        float_T u2 = randomFloat();

        auto [ax, ay] = calculateAnisotropicParams(roughness, anisotropic);

        // Transform V to tangent space
        // TODO what was this for?
        //Vec3 Vt = Vec3(V.dot(X), V.dot(Y), V.dot(N));

        // Sample visible normal distribution
        float_T phi = 2.0f * PI * u1;
        float_T theta = std::atan(roughness * std::sqrt(u2 / (1.0f - u2)));

        float_T cos_theta = std::cos(theta);
        float_T sin_theta = std::sin(theta);
        float_T cos_phi = std::cos(phi);
        float_T sin_phi = std::sin(phi);

        // Compute half vector in tangent space
        Vec3 Hlocal = Vec3(
            sin_theta * cos_phi,
            sin_theta * sin_phi,
            cos_theta
        ).normalized();

        // Unstretch
        Hlocal = Vec3(Hlocal.x * ax, Hlocal.y * ay, Hlocal.z).normalized();

        // Transform back to world space
        return (X * Hlocal.x + Y * Hlocal.y + N * Hlocal.z).normalized();
    }

    // Geometric shadowing functions
    static float_T smithG_GGX_aniso(float_T NdotV, float_T VdotX, float_T VdotY, float_T NdotL, float_T LdotX, float_T LdotY,
            float_T ax, float_T ay) {
        float_T lambda_V = NdotV + safe_sqrt(sqr(VdotX*ax) + sqr(VdotY*ay) + sqr(NdotV));
        float_T lambda_L = NdotL + safe_sqrt(sqr(LdotX*ax) + sqr(LdotY*ay) + sqr(NdotL));

        return 2. / (lambda_V * lambda_L);
    }

    static float_T smithG(float_T NdotV, float_T alphaG) {
        float_T a = alphaG * alphaG;
        float_T b = NdotV * NdotV;
        return 1. / (NdotV + safe_sqrt(a + b - a * b));
    }

    // TODO clean up all of the clamps and epsilons


public:
    struct BRDFSample {
        Vec3 direction;  // Sampled direction in world space
        float_T pdf;    // Probability density of the sample
    };

    // Main sampling function
    BRDFSample sampleBRDF(const Vec3& V, const Vec3& N, const Vec3& X, const Vec3& Y) const {
        BRDFSample sample;

        float_T rand = randomFloat();
        
        // Sample direction based on weights
        if (rand < diffuseWeight) {
            // Cosine-weighted hemisphere sampling
            float_T r1 = randomFloat();
            float_T r2 = randomFloat();
            float_T phi = 2.0f * PI * r1;
            float_T cosTheta = std::sqrt(r2);
            float_T sinTheta = std::sqrt(1.0f - r2);
            
            sample.direction = (
                X * (std::cos(phi) * sinTheta) +
                Y * (std::sin(phi) * sinTheta) +
                N * cosTheta
            ).normalized();
        } else if (rand < diffuseWeight + specularWeight) {
            Vec3 H = sampleGGX(roughness * roughness, X, Y, N);
            sample.direction = (-V).reflect(H);
        } else {
            Vec3 H = sampleGGX(fixedClearcoatRoughness, X, Y, N);
            sample.direction = (-V).reflect(H);
        }

        const Vec3& L = sample.direction;

        // half vector: half-between V and L
        const Vec3& H = (V + L).normalized();

        // use lambdas to make clear what information each calculation is using (via their captures) - useful for readability and debugging

        float_T diffusePdf = [&N, &L] {
            // For cosine-weighted hemisphere sampling, the PDF is cos(theta)/pi
            // where theta is the angle between the normal and sampled direction
            float_T cosTheta = std::max(epsilon, N.dot(L));
            return cosTheta / PI;
        }();

        // needed for both specular and clearcoat PDFs
        float_T NdotH = std::max(epsilon, N.dot(H));
        float_T VdotH = std::max(epsilon, V.dot(H));

        float_T specularPdf = [&H, &N, &X, &Y, &NdotH, &VdotH, this]{
            auto [ax, ay] = calculateAnisotropicParams(roughness, anisotropic);

            // Calculate the GGX distribution term
            float_T D = D_GGX_aniso(H, N, X, Y, ax, ay);

            // The PDF for GGX importance sampling is:
            // pdf = D * NdotH / (4 * VdotH)
            // This comes from the Jacobian of the half-vector transformation

            if (VdotH < epsilon || NdotH < epsilon) {
                return 0.0f;
            }

            float_T pdf = (D * NdotH) / (4.0f * VdotH);

            return std::clamp(pdf, epsilon, 1e8f);
        }();

        float_T clearcoatPdf = [&VdotH, &NdotH, this]{
            // Clearcoat uses GTR1 distribution with fixed roughness
            // interpolated based on clearcoatGloss
            float_T alpha = std::lerp(0.1f, 0.001f, clearcoatGloss);

            // D_GTR1 term
            float_T D = GTR1(NdotH, alpha);

            if (VdotH < epsilon || NdotH < epsilon) {
                return 0.0f;
            }

            // Same jacobian as specular
            float_T pdf = D * NdotH / (4.0f * VdotH);
            return std::clamp(pdf, epsilon, 1e8f);
        }();

        // Combine PDFs using balance heuristic
        sample.pdf = diffuseWeight * diffusePdf + 
                    specularWeight * specularPdf + 
                    clearcoatWeight * clearcoatPdf;

        return sample;
    }

    // TODO rename/document params (also for sample)

    // Combined BRDF evaluation
    Vec3 evaluateBRDF(const Vec2& textureCoords, const Vec3& V, const Vec3& L, const Vec3& X, const Vec3& Y, const Vec3& N) const {
        float_T NdotL = N.dot(L);
        float_T NdotV = N.dot(V);
        
        if (NdotL <= 0.0f || NdotV <= 0.0f)
            return Vec3(0.0f);

        // round anything below epsilon to epsilon
        NdotL = std::max(epsilon, NdotL);
        NdotV = std::max(epsilon, NdotV);

        Vec3 H = (L + V).normalized();
        float_T LdotH = std::max(epsilon, L.dot(H));
        
        Vec3 baseColor = diffuseColorAtTextureCoords(textureCoords);

        auto diffuseBRDFContribution = [&]{
            float_T FL = std::pow(1.0f - NdotL, 5.0f);
            float_T FV = std::pow(1.0f - NdotV, 5.0f);
            float_T Rr = 2.0f * roughness * sqr(LdotH);

            float_T Fd90 = 0.5f + 2.0f * roughness * sqr(LdotH);
            float_T Fd = std::lerp(1.0f, Fd90, FL) * std::lerp(1.0f, Fd90, FV);

            float_T Fss90 = Rr;
            float_T Fss = std::lerp(1.0f, Fss90, FL) * std::lerp(1.0f, Fss90, FV);
            float_T ss = 1.25f * (Fss * (1.0f / (NdotL + NdotV) - 0.5f) + 0.5f);

            return (1.0f / PI) * std::lerp(Fd, ss, subsurface) * baseColor * baseColorDiffuseIntensity * (1.0f - metallic);
        }();

        auto specularBRDFContribution = [&] {
            // Calculate anisotropic roughness parameters
            auto [ax, ay] = calculateAnisotropicParams(roughness, anisotropic);

            // Calculate tint color
            Vec3 tint = (luminance(baseColor) > epsilon) ? 
                baseColor / luminance(baseColor) : Vec3(1.0f);

            // Calculate specular color with proper metallic workflow
            Vec3 specularColor = (Vec3(0.08)  * specular)
                .lerp(tint, specularTint)
                .lerp(baseColor, metallic);

            // Also calculate sheen contribution (disney sheen)
            Vec3 sheenColor = Vec3(1.).lerp(tint, sheenTint);
            Vec3 sheenContribution = sheen * sheenColor * schlickFresnel(0., LdotH);

            // Fundamentally microfacet based:

            // Fresnel term
            Vec3 F = schlickFresnel(specularColor, LdotH);

            // Distribution term (GGX/Trowbridge-Reitz)
            float_T D = D_GGX_aniso(H, N, X, Y, ax, ay);

            // Geometric term
            float_T G = smithG_GGX_aniso(NdotV, V.dot(X), V.dot(Y),
                    NdotL, L.dot(X), L.dot(Y),
                    ax, ay);

            Vec3 specularBRDF = F * D * G;

            return specularBRDF + sheenContribution;
        }();

        auto clearcoatBRDFContribution = [&] {
            float_T NdotH = std::max(epsilon, N.dot(H));

            // Fixed clearcoat IOR of 1.5 gives F0 of 0.04
            const float_T F0 = 0.04f;
            // Use the same Fresnel equation as specular but with fixed F0
            float_T Fr = schlickFresnel(F0, LdotH);

            // TODO decide

            // Option 1: Keep GTR1 with better gloss mapping
            float_T alpha = std::lerp(0.1f, 0.001f, clearcoatGloss);
            float_T Dr = GTR1(NdotH, alpha);

            // Option 2: Use GGX (like specular) with fixed roughness
            //float_T roughness = std::lerp(fixedClearcoatRoughness, 0.1f, clearcoatGloss);
            //float_T Dr = D_GGX_aniso(H, N, X, Y, roughness, roughness);

            // Use the same geometry term as specular but with fixed roughness
            float_T Gr = smithG(NdotL, fixedClearcoatRoughness) * smithG(NdotV, fixedClearcoatRoughness);

            return Vec3(clearcoat * Gr * Fr * Dr);
        }();
        
        return diffuseBRDFContribution + specularBRDFContribution + clearcoatBRDFContribution;
    }
};

/// for phong rendering, the mateiral is always a PhongMaterial, for pathtracing, it is always a BRDFMaterial
struct Material {
    std::variant<PhongMaterial, PrincipledBRDFMaterial> variant;

    const PhongMaterial& assumePhongMaterial() const {
        assert(std::holds_alternative<PhongMaterial>(variant) && "Material is not a Phong material");
        return std::get<PhongMaterial>(variant);
    }

    const PrincipledBRDFMaterial& assumePrincipledBRDFMaterial() const {
        assert(std::holds_alternative<PrincipledBRDFMaterial>(variant) && "Material is not a BRDF material");
        return std::get<PrincipledBRDFMaterial>(variant);
    }

    bool hasTexture() const {
        return std::visit([&](auto&& object){
            return object.texture.has_value();
        }, variant);
    }
};

struct Intersection{
    // ray that caused the intersection
    const Ray* incomingRay;
    // TODO instead of storing the intersection point, could store the distance along the ray
    Vec3 point;
    Vec3 surfaceNormal;
    const Material* material;
    Vec2 textureCoords;

    Intersection(const Ray* incomingray, Vec3 point, Vec3 surfaceNormal, const Material* material, Vec2 textureCoords)
        : incomingRay(incomingray), point(point), surfaceNormal(surfaceNormal), material(material), textureCoords(textureCoords){
        assert(surfaceNormal == surfaceNormal.normalized() && "Surface normal must be normalized");
        assert(incomingRay && "Incoming ray must not be null");
        assert(material && "Material must not be null");
    }

    float_T distance() const{
        return (point - incomingRay->origin).length();
    }

};

struct Sphere {
    Vec3 center;
    float_T radius;
    Material material;

    Sphere(Vec3 center, float_T radius, Material material)
        : center(center), radius(radius), material(std::move(material)){ }

    std::optional<Intersection> intersect(const Ray& ray) const {
        // mostly generated by ai

        // Vector from the ray's origin to the sphere's center
        Vec3 oc = ray.origin - center;

        // Coefficients of the quadratic equation (a*t^2 + b*t + c = 0)
        float_T a = ray.direction.dot(ray.direction);               // a = D•D
        float_T b = 2.0 * oc.dot(ray.direction);                    // b = 2 * oc•D
        float_T c = oc.dot(oc) - radius * radius;                   // c = (oc•oc - r^2)

        // Discriminant of the quadratic equation
        float_T discriminant = b * b - 4 * a * c;

        // No intersection if the discriminant is negative
        if (discriminant < 0) {
            return std::nullopt;
        }

        // Calculate the two intersection distances along the ray
        float_T sqrtDiscriminant = std::sqrt(discriminant);
        float_T t1 = (-b - sqrtDiscriminant) / (2.0 * a);
        float_T t2 = (-b + sqrtDiscriminant) / (2.0 * a);


        // Choose the closest intersection point in front of the ray origin
        float_T t = (t1 > 0) ? t1 : ((t2 > 0) ? t2 : -1);
        // if both are behind the ray, return no intersection
        if (t < 0) {
            return std::nullopt;
        }

        // Calculate intersection details
        Vec3 intersectionPoint = ray.origin + ray.direction * t;
        Vec3 intersectionNormal = (intersectionPoint - center).normalized();

        // Calculate texture coordinates
        Vec3 localPoint = (intersectionPoint - center) / radius; // Normalize to unit sphere

        // Calculate spherical coordinates
        float_T theta = std::atan2(localPoint.z, localPoint.x); // Longitude
        float_T phi = std::acos(localPoint.y);                  // Latitude

        // Map spherical coordinates to texture coordinates (u, v)
        float_T u = (theta + PI) / (2 * PI);
        float_T v = phi / PI;

        return Intersection(&ray, std::move(intersectionPoint), std::move(intersectionNormal), &material, Vec2(u, v));
    }

    /// equality (explicitly do not check for material, because two spheres with the exact same position and geometry should not have different materials; this is the same for all other shapes)
    bool operator==(const Sphere& other) const {
        return center == other.center && radius == other.radius;
    }
};

struct Cylinder {
    Vec3 center;
    float_T radius;
    float_T eachSideHeight;
    Vec3 axis;
    Material material;

    // stretch the textures at the side of the 
    float_T textureSideStretchFactor;
    float_T textureCapScale;

    Cylinder(Vec3 center, float_T radius, float_T height, Vec3 axis, Material material, float_T textureSideStretchFactor = 1.0, float_T textureCapScale = 1.0)
        : center(center), radius(radius), eachSideHeight(height), axis(axis), material(std::move(material)), textureSideStretchFactor(textureSideStretchFactor), textureCapScale(textureCapScale) {  }

    std::optional<Intersection> intersect(const Ray& ray) const {
        // mostly generated by ai

        Vec3 d = ray.direction - axis * ray.direction.dot(axis);  // Projected ray direction onto the cylinder's plane
        Vec3 oc = ray.origin - center;
        Vec3 oc_proj = oc - axis * oc.dot(axis);                  // Projected ray origin onto the cylinder's plane

        float_T a = d.dot(d);
        float_T b = 2. * d.dot(oc_proj);
        float_T c = oc_proj.dot(oc_proj) - radius * radius;
        std::optional<Intersection> closestIntersection = std::nullopt;

        // Quadratic discriminant for side wall intersection
        float_T discriminant = b * b - 4 * a * c;
        if (discriminant >= 0) {
            float_T sqrtDiscriminant = std::sqrt(discriminant);
            for (float_T t : { (-b - sqrtDiscriminant) / (2.0 * a), (-b + sqrtDiscriminant) / (2.0 * a) }) {
                if (t < 0) continue;

                Vec3 point = ray.origin + ray.direction * t;
                Vec3 localPoint = point - center;
                float_T projectionOnAxis = localPoint.dot(axis);

                // Check if intersection point is within height limits of the cylinder
                if (projectionOnAxis >= -eachSideHeight && projectionOnAxis <= eachSideHeight) {
                    Vec3 normal = (localPoint - axis * projectionOnAxis).normalized();
                    Intersection intersection(&ray, point, normal, &material, textCoordsOfSideIntersection(point));


                    // Update closest intersection
                    if (!closestIntersection || t < (closestIntersection->point - ray.origin).length()) {
                        closestIntersection = intersection;
                    }
                }
            }
        }

        auto checkCapIntersection = [&](const Vec3& capCenter, const Vec3& capNormal) -> std::optional<Intersection> {
            float_T denom = ray.direction.dot(capNormal);
            if (std::abs(denom) < 1e-6) return std::nullopt;

            float_T tCap = (capCenter - ray.origin).dot(capNormal) / denom;
            if (tCap < 0) return std::nullopt;

            Vec3 point = ray.origin + ray.direction * tCap;
            if ((point - capCenter).length() <= radius) {  // Check if within radius of cap
                Intersection intersection(&ray, point, capNormal, &material, textCoordsOfCapIntersection(point));
                return intersection;
            }
            return std::nullopt;
        };

        // Check intersections with the base and top caps
        for (auto& cap : { std::make_pair(center - axis * eachSideHeight, -axis), 
                std::make_pair(center + axis * eachSideHeight, axis) }) {
            if (auto capIntersection = checkCapIntersection(cap.first, cap.second); capIntersection) {
                float_T capDistance = (capIntersection->point - ray.origin).length();
                if (!closestIntersection || capDistance < (closestIntersection->point - ray.origin).length()) {
                    closestIntersection = capIntersection;
                }
            }
        }

        return closestIntersection;
    }

    bool operator==(const Cylinder& other) const {
        return center == other.center && radius == other.radius && eachSideHeight == other.eachSideHeight && axis == other.axis;
    }

private:
    Vec2 textCoordsOfSideIntersection(const Vec3& intersectionPoint) const {
        Vec3 baseToIntersection = intersectionPoint - (center - axis * eachSideHeight);
        float_T vPosAlongAxis = baseToIntersection.dot(axis);        
        float_T v = vPosAlongAxis / (2 * eachSideHeight * textureSideStretchFactor);  // Map height position to v in [0, 1], then stretch



        Vec3 circumferentialDir = (baseToIntersection - axis * vPosAlongAxis).normalized();
        float_T theta = std::atan2(circumferentialDir.z, circumferentialDir.x);        
        float_T u = (theta + PI) / (2 * PI);  // Map angle to u in [0, 1]

        return Vec2(u,v);
    }

    Vec2 textCoordsOfCapIntersection(const Vec3& intersectionPoint) const {
        // Determine which cap (top or bottom) we're on based on axis direction and height
        Vec3 capCenter = intersectionPoint.dot(axis) > 0 ? (center + axis * eachSideHeight) : (center - axis * eachSideHeight);
        Vec3 localCapPoint = intersectionPoint - capCenter;

        // Map `localCapPoint` to polar coordinates within the cap radius
        float_T r = localCapPoint.length() / radius;  // Distance from center mapped to [0, 1]
        float_T capTheta = std::atan2(localCapPoint.z, localCapPoint.x);

        float_T u = 0.5 + r * std::cos(capTheta) / 2;  // Map radial distance and angle to texture u
        float_T v = 0.5 + r * std::sin(capTheta) / 2;  // Map radial distance and angle to texture v

        return Vec2(u,v) / textureCapScale;
    }
};

template<bool hasVertexNormals>
struct Triangle {
    Vec3 v0,v1,v2;
    // TODO could try deduplicating materials for triangle objects later on, for big meshes that all have the same material
    Material material;
    // these are only valid if the material has a texture
    Vec2 v0TexCoord, v1TexCoord, v2TexCoord;

    // TODO could precompute bounding box or mins/maxes for faster building of the BVH

    // depending on the template parameter either store a single normal for the whole face or one for each vertex
    struct VertexNormals{
        Vec3 v0Normal, v1Normal, v2Normal;
    };
    std::conditional_t<!hasVertexNormals,
        // normal that's constant across the face
        Vec3,
        // normal for each vertex
        VertexNormals
    > normalInfo;

    Triangle(Vec3 v0, Vec3 v1, Vec3 v2, Material material, Vec2 v0TexCoord, Vec2 v1TexCoord, Vec2 v2TexCoord)
        requires(!hasVertexNormals)
        : v0(v0), v1(v1), v2(v2), material(std::move(material)), v0TexCoord(v0TexCoord), v1TexCoord(v1TexCoord), v2TexCoord(v2TexCoord),
            normalInfo((v1 - v0).cross(v2 - v0).normalized()) { }

    Triangle(Vec3 v0, Vec3 v1, Vec3 v2, Material material, Vec2 v0TexCoord, Vec2 v1TexCoord, Vec2 v2TexCoord, VertexNormals vertexNormals)
        requires(hasVertexNormals)
        : v0(v0), v1(v1), v2(v2), material(std::move(material)), v0TexCoord(v0TexCoord), v1TexCoord(v1TexCoord), v2TexCoord(v2TexCoord),normalInfo(vertexNormals){ }

    // TODO normal vector interpolation at any point for smooth shading (requires knowledge of the rest of the mesh)

    std::optional<Intersection> intersect(const Ray& ray) const {
        // Möller–Trumbore intersection
        // mostly generated by ai
        Vec3 edge1 = v1 - v0;
        Vec3 edge2 = v2 - v0;
        Vec3 h = ray.direction.cross(edge2);
        float_T a = edge1.dot(h);

        // If a is near zero, the ray is parallel to the triangle
        if (std::abs(a) < epsilon) return std::nullopt;

        float_T f = 1.0 / a;
        Vec3 s = ray.origin - v0;
        float_T u = f * s.dot(h);

        // Check if the intersection is outside the triangle
        if (u < 0.0 || u > 1.0) return std::nullopt;

        Vec3 q = s.cross(edge1);
        float_T v = f * ray.direction.dot(q);

        // Check if the intersection is outside the triangle
        if (v < 0.0 || u + v > 1.0) return std::nullopt;

        // Calculate the distance along the ray to the intersection point
        float_T t = f * edge2.dot(q);

        // Only accept intersections that are in front of the ray origin
        if (t > epsilon) {
            Vec3 intersectionPoint = ray.origin + ray.direction * t;

            // interpolate texture coordinates
            // calculate barycentric coordinate `w`
            float_T w = 1. - u - v;

            // interpolate the texture coordinates using barycentric weights
            Vec2 interpolatedTexCoord = v0TexCoord * w + v1TexCoord * u + v2TexCoord * v;

            Vec3 normal;
            if constexpr(!hasVertexNormals){
                // in this case, the normalInfo just holds the face normal
                normal = normalInfo;
            }else{
                // in this case, the normalInfo holds the normals for each vertex
                // -> interpolate the normals using barycentric coordinates (can reuse them from the texture coordinate calculation)
                normal = (normalInfo.v0Normal * w + normalInfo.v1Normal * u + normalInfo.v2Normal * v).normalized();
            }

            // ensure the normal points against the ray's direction,
            // we want to make sure that backfaces look like frontfaces
            if (normal.dot(ray.direction) > 0) {
                normal = -normal;
            }

            return Intersection(&ray, intersectionPoint, normal, &material, interpolatedTexCoord);
        }

        return std::nullopt;
    }
    
    bool operator==(const Triangle<hasVertexNormals>& other) const{
        return v0 == other.v0 && v1 == other.v1 && v2 == other.v2;
    }
};

// separating out both types
// - avoids checking which type the triangle is for every intersection at runtime, thus improves performance
//      - these checks are moved to the std::visit which is more efficient and better for branch prediction
// - (if we used indirections instead of std variant would reduce memory usage)
using TriangleWithVertexNormals = Triangle<true>;
using TriangleWithConstantFaceNormal = Triangle<false>;

struct SceneObject {
    // use a variant to store different types of objects, to avoid virtual function calls (expensive)
    std::variant<TriangleWithVertexNormals, TriangleWithConstantFaceNormal, Sphere, Cylinder> variant;

    std::optional<Intersection> intersect(const Ray& ray) const {
        return std::visit([&](auto&& object){
            return object.intersect(ray);
        }, variant);
    }

    bool operator==(const SceneObject& other) const{
        return variant == other.variant;
    }
};

struct BoundingBox{
    Vec3 min, max;

    BoundingBox() :
          min(Vec3(std::numeric_limits<float_T>::max()))
        , max(Vec3(-std::numeric_limits<float_T>::max())) 
    { }

    /// assumes min and max are actually <= each other, componentwise
    BoundingBox(Vec3 min, Vec3 max)
        : min(min), max(max){
        assert(min.x <= max.x && min.y <= max.y && min.z <= max.z && "Trying to construct invalid bounding box");
    }

    // TODO look at the min/max 0 things, there has to be a better way
    explicit BoundingBox(SceneObject object): min(0.), max(0.){
        std::visit([this](auto&& object){
            using T = std::decay_t<decltype(object)>;
            if constexpr(std::is_same_v<T, TriangleWithVertexNormals> || std::is_same_v<T, TriangleWithConstantFaceNormal>){
                min = Vec3(
                    std::min({object.v0.x, object.v1.x, object.v2.x}),
                    std::min({object.v0.y, object.v1.y, object.v2.y}),
                    std::min({object.v0.z, object.v1.z, object.v2.z})
                );
                max = Vec3(
                    std::max({object.v0.x, object.v1.x, object.v2.x}),
                    std::max({object.v0.y, object.v1.y, object.v2.y}),
                    std::max({object.v0.z, object.v1.z, object.v2.z})
                );
            }else if constexpr(std::is_same_v<T, Sphere>){
                min = object.center - Vec3(object.radius);
                max = object.center + Vec3(object.radius);
            }else if constexpr(std::is_same_v<T, Cylinder>){
                // cylinder can be encompassed in a box with one corner at one side of the bottom cap, and the other at the other side of the top cap
                const Vec3 bottomCapCenter = object.center - object.axis * object.eachSideHeight;
                const Vec3 topCapCenter = object.center + object.axis * object.eachSideHeight;

                // we'll be shifting points along the 2 "cap" axes (by the radius), so mask out the "height" axis of the cylinder
                const Vec3 axisMask = Vec3(1.) - object.axis;
                // and then invert the axis for one corner, and use it directly for the other
                const Vec3 bottomCapCorner = bottomCapCenter - axisMask * Vec3(object.radius);
                const Vec3 topCapOppositeCorner = topCapCenter + axisMask * Vec3(object.radius);

                min = bottomCapCorner;
                max = topCapOppositeCorner;
            }else{
                static_assert(false, "Unexpected object type");
            }
        }, object.variant);
        assert(min.x <= max.x && min.y <= max.y && min.z <= max.z && "Internal error: invalid bounding box constructed");
    }

    Vec3 center() const { 
        return (min + max) * 0.5; 
    }

    Vec3 extent() const { 
        return max - min; 
    }

    BoundingBox merge(const BoundingBox& other) const {
        return BoundingBox(
            min.min(other.min),
            max.max(other.max)
        );
    }

    bool contains(const Vec3& point) const {
        return point.x >= min.x && point.x <= max.x &&
               point.y >= min.y && point.y <= max.y &&
               point.z >= min.z && point.z <= max.z;
    }

    /*
       TODO probably either remove or merge this with the intersects function
    float_T intersection_distance(const Ray& ray) const {
        Vec3 invDir = Vec3(1.) / ray.direction;

        Vec3 t0 = (min - ray.origin) * invDir;
        Vec3 t1 = (max - ray.origin) * invDir;

        Vec3 tmin = t0.min(t1);
        Vec3 tmax = t0.max(t1);

        float_T tenter = std::max(std::max(tmin.x, tmin.y), tmin.z);
        float_T texit = std::min(std::min(tmax.x, tmax.y), tmax.z);

        return tenter <= texit && texit >= 0 ? tenter : std::numeric_limits<float_T>::max();
    }
    */

    bool intersects(const Ray& ray) const {
        Vec3 invDir = Vec3(1.) / ray.direction;
        
        Vec3 t0 = (min - ray.origin) * invDir;
        Vec3 t1 = (max - ray.origin) * invDir;
        
        Vec3 tmin = t0.min(t1);
        Vec3 tmax = t0.max(t1);
        
        float_T tenter = std::max(std::max(tmin.x, tmin.y), tmin.z);
        float_T texit = std::min(std::min(tmax.x, tmax.y), tmax.z);
        
        return tenter <= texit && texit >= 0;
    }

    bool overlaps(const BoundingBox& other) const {
        return min.x <= other.max.x && max.x >= other.min.x &&
               min.y <= other.max.y && max.y >= other.min.y &&
               min.z <= other.max.z && max.z >= other.min.z;
    }

    float_T surface_area() const {
        Vec3 d = extent();
        return 2. * (d.x * d.y + d.y * d.z + d.z * d.x);
    }
};

/// bounding volume hierarchy
struct BVHNode{
    struct ObjectRange{
        /// left-inclusive, right-exclusive
        std::pair<size_t, size_t> objectRange;

        // allow implicit conversion
        ObjectRange(std::pair<size_t, size_t> objectRange)
            : objectRange(objectRange){ }

        ObjectRange(size_t first, size_t last)
            : objectRange(std::make_pair(first, last)){ }

        size_t size() const{
            return objectRange.second - objectRange.first;
        }

        bool empty() const{
            return objectRange.first == objectRange.second;
        }

        bool operator==(const ObjectRange& other) const{
            return objectRange == other.objectRange;
        }

        size_t begin() const{
            return objectRange.first;
        }

        size_t end() const{
            return objectRange.second;
        }

        // implicit conversion to pair
        operator std::pair<size_t, size_t>() const{
            return objectRange;
        }
    };

    BoundingBox bounds;
    std::unique_ptr<BVHNode> left, right;
    /// range of objects in the scene object list that this node represents
    ObjectRange objectRange;

    static constexpr size_t MAX_DEPTH = 16;
    // TODO maybe remove in future, or make an option; not in use currently, because even though it reduces memory usage, it doesn't improve performance
    //static constexpr size_t MIN_OBJECTS = 4;

public:
    /// REORDERS THE OBJECTS VECTOR
    /// but does not store it anywhere, the objects explicitly live outside the BVH
    BVHNode(ObjectRange objectRangeP, std::vector<SceneObject>& objects, uint32_t depth = 0) 
        : objectRange(std::move(objectRangeP))
    {
        assert(objectRange.begin() <= objectRange.end() && "Invalid objectRange");
        assert(objectRange.end() <= objects.size() && "objectRange exceeds object vector");

        // get the maximum bounds of all objects in the range
        bounds = boundsFromObjectRange(objectRange, objects);
        
        if (depth >= MAX_DEPTH)
            return;

        // Find the axis with greatest extent, to be able to subdivide as equally as possible
        Vec3 extent = bounds.extent();
        int splitAxis = 0;
        if (extent.y > extent.x) splitAxis = 1;
        if (extent.z > extent[splitAxis]) splitAxis = 2;

        // idea of partitioning via centroids by ai

        // partition objects along the median of the split axis
        auto begin = objects.begin() + objectRange.begin();
        auto end = objects.begin() + objectRange.end();
        std::nth_element(begin, begin + (end - begin)/2, end,
            [splitAxis](const SceneObject& a, const SceneObject& b) {
                return computeCentroid(a)[splitAxis] < computeCentroid(b)[splitAxis];
            }
        );
        size_t midIndex = objectRange.begin() + (objectRange.end() - objectRange.begin()) / 2;
        
        // only create children if there are actually objects in the range
        if (midIndex > objectRange.begin() && midIndex < objectRange.end()) {
            left = std::make_unique<BVHNode>(ObjectRange(objectRange.begin(), midIndex), objects, depth + 1);
            right = std::make_unique<BVHNode>(ObjectRange(midIndex, objectRange.end()), objects, depth + 1);
        }
    }

    std::optional<Intersection> intersect(const Ray& ray, const std::vector<SceneObject>& objects) const {
        if (!bounds.intersects(ray))
            return std::nullopt;

        if (isLeaf()) {
            auto closestIntersection = std::optional<Intersection>();

            for (auto i = objectRange.begin(); i < objectRange.end(); ++i)
                if (auto intersection = objects[i].intersect(ray))
                    if(!closestIntersection.has_value() || intersection->distance() < closestIntersection->distance())
                        closestIntersection = *intersection;

            return closestIntersection;
        }

        // here, we're just checking both boxes
        // TODO but we could check the least number of nodes by:
        // a checking the closer box first
        // b only checking the other box if the boxes overlap, or if the closer box has no intersection

        auto leftIntersection = left->intersect(ray, objects);
        auto rightIntersection = right->intersect(ray, objects);
        if(leftIntersection.has_value() && rightIntersection.has_value())
            return leftIntersection->distance() < rightIntersection->distance() ? leftIntersection : rightIntersection;
        else if(leftIntersection.has_value())
            // don't std::move this, to allow copy elision
            return leftIntersection;
        else
            // if the right intersection is empty, this will also return nullopt
            return rightIntersection;
    }

    bool isLeaf() const { 
        return !left && !right; 
    }

    // === The following public methods are all for debugging purposes ===

    void verifyBVH(const std::vector<SceneObject>& objects) const {
#ifdef NDEBUG
        std::println(stderr, "verifyBVH should only be called in debug mode");
        std::abort();
#endif
        // Verify range validity
        assert(objectRange.begin() <= objectRange.end() && "Invalid objectRange");
        assert(objectRange.end() <= objects.size() && "objectRange exceeds vector size");

        if (!isLeaf()) {
            // Verify children exist
            // (this is a bit stupid, because isLeaf would prob fail first, but just in case)
            assert(left && right && "Non-leaf node missing children");

            // Recursively verify children
            left->verifyBVH(objects);
            right->verifyBVH(objects);

            // Verify child objectRanges overlap linearly, i.e.
        }
    }

    void recursivelyCollectIntersectedBoxes(const Ray& ray, std::vector<std::pair<BoundingBox, int>>& boxes, int depth = 0) const {
        if (bounds.intersects(ray)) {
            boxes.push_back({bounds, depth});
            
            if (!isLeaf()) {
                left->recursivelyCollectIntersectedBoxes(ray, boxes, depth + 1);
                right->recursivelyCollectIntersectedBoxes(ray, boxes, depth + 1);
            }
        }
    }

    uint64_t numNodes() const {
        if (isLeaf())
            return 1;
        return 1 + left->numNodes() + right->numNodes();
    }

private:
    static Vec3 computeCentroid(const SceneObject& object) {
        // partially generated by ai
        return std::visit([](const auto& obj) -> Vec3 {
            using T = std::decay_t<decltype(obj)>;
            if constexpr (std::is_same_v<T, TriangleWithVertexNormals> || std::is_same_v<T, TriangleWithConstantFaceNormal>){
                return (obj.v0 + obj.v1 + obj.v2) / 3.;
            } else if constexpr (std::is_same_v<T, Sphere>) {
                return obj.center;
            } else if constexpr (std::is_same_v<T, Cylinder>) {
                return obj.center;
            } else{
                static_assert(false, "Unexpected object type");
            }
        }, object.variant);
    }

    static BoundingBox boundsFromObjectRange(ObjectRange range, 
                                    const std::vector<SceneObject>& objects) {
        BoundingBox bounds;
        for (size_t i = range.begin(); i < range.end(); ++i) {
            bounds = bounds.merge(BoundingBox(objects[i]));
        }
        return bounds;
    }
};

struct PointLight{
    Vec3 position;
    // the json files seem to integrate intensity and color into one vector
    Vec3 intensityPerColor;
    /// 0 shadow softness means copletely hard shadows, higher means softer
    /// ONLY AFFECTS PATHTRACED SHADOWS
    float_T shadowSoftness;
};

enum class RenderMode{
    BINARY,
    PHONG,
    DEBUG_BVH,
    DEBUG_NORMALS,
    PATHTRACE,
    // continue rendering after rendering the first frame, and average the results
    PATHTRACE_INCREMENTAL,
};

enum class ToneMapMode{
    // local linear is the default, see `localLinearToneMapping` for why
    LOCAL_LINEAR,  // see `localLinearToneMapping`
    GLOBAL_LINEAR, // see `globalLinearToneMapping`
};

struct Scene{
    uint32_t nBounces;
    RenderMode renderMode;
    std::unique_ptr<Camera> camera;
    Vec3 backgroundColor;

    ToneMapMode toneMapMode;

    std::vector<PointLight> pointLights;

    std::vector<SceneObject> objects;

    struct {
        uint32_t samplesPerPixel;
        uint32_t apertureSamplesPerPixelSample;
        uint32_t pointLightsamplesPerBounce;
        /// be aware, that these are the samples that will result in exponentially more rays
        /// there is almost no reason to use this higher than 1, more samples per pixel is the better monte carlo answer to this
        uint32_t hemisphereSamplesPerBounce;
    } pathtracingSamples;

    Scene(uint32_t nBounces,
            RenderMode renderMode,
            std::unique_ptr<Camera> camera,
            Vec3 backgroundColor,
            ToneMapMode toneMapMode,
            std::vector<PointLight> lights,
            std::vector<SceneObject> objects,
            uint32_t pathtracingSamplesPerPixel,
            uint32_t pathtracingApertureSamplesPerPixelSample,
            uint32_t pathtracingPointLightsamplesPerBounce,
            uint32_t pathtracingHemisphereSamplesPerBounce)
        : nBounces(nBounces),
        renderMode(renderMode),
        camera(std::move(camera)),
        backgroundColor(backgroundColor),
        toneMapMode(toneMapMode),
        pointLights(std::move(lights)),
        objects(std::move(objects)),
        pathtracingSamples(
            pathtracingSamplesPerPixel,
            pathtracingApertureSamplesPerPixelSample,
            pathtracingPointLightsamplesPerBounce,
            pathtracingHemisphereSamplesPerBounce
        ){ }

    /// assumes that there is at least one point light in the scene
    const PointLight& randomPointLight() const {
        assert(!pointLights.empty() && "No point lights in scene");
        // std::rand() % pointLights.size() literally triples render time, so use this instead
        return pointLights[randomFloat() * pointLights.size()];
    }
};


struct Renderer{
    Scene scene;
    PPMWriter writer;
    // ordered the same way a PPM file is, row by row
    // use a buffer instead of writing to the file immediately to be able to do it in parallel
    std::vector<Vec3> hdrPixelBuffer;

    BVHNode bvh;

    Renderer(Scene&& scene, std::string_view outputFilePath)
        : scene(std::move(scene)),
          writer(outputFilePath, this->scene.camera->widthPixels, this->scene.camera->heightPixels),
          hdrPixelBuffer(this->scene.camera->widthPixels*this->scene.camera->heightPixels, Vec3(0.)),
          // this reorders the objects in the scene to be able to build the BVH
          bvh(BVHNode(BVHNode::ObjectRange(0, this->scene.objects.size()), this->scene.objects))
    {}

    // all default values as an overview
    struct Defaults {
        static constexpr std::string outputFilePath       = "out.ppm";
        static constexpr uint32_t nBounces                = 4;
        static constexpr ToneMapMode toneMapMode          = ToneMapMode::LOCAL_LINEAR;
        static constexpr float_T pointLightShadowSoftness = 0.;
        static constexpr float_T emissiveness             = 0.;
        static constexpr PhongMaterial defaultPhongMaterial   = PhongMaterial(Vec3(1,1,1), Vec3(1,1,1), 0.5, 0.5, 32, 0.0, 0.0, std::nullopt);
        static constexpr PrincipledBRDFMaterial defaultPrincipledBRDFMaterial = PrincipledBRDFMaterial(Vec3(1.), std::nullopt, 0., 1., 0., 0., 0.5, 1., 0., 1., 0., 0., 0., 0.);

    };

    void bufferSpecificPixel(Vec2 pixel, Vec3 color){
        assert(pixel.x >= 0 && pixel.x < scene.camera->width && pixel.y >= 0 && pixel.y < scene.camera->height && "Pixel out of range");
        assert(scene.camera->width * pixel.y + pixel.x < hdrPixelBuffer.size() && "Pixel out of range");
        hdrPixelBuffer[pixel.y * scene.camera->width + pixel.x] = color;
    }

    void writeBufferToFile(){
        // start where the pixel data starts
        writer.rewind();

        assert(hdrPixelBuffer.size() == scene.camera->widthPixels * scene.camera->heightPixels && "Pixel buffer size mismatch");
        for(auto& hdrPixel: hdrPixelBuffer){
            // second part of tone-mapping
            auto ldrPixel = hdrPixel.clamp(0., 1.);
            writer.writePixel(ldrPixel);
        }
        writer.flush();
    }


    bool isInShadow(const Intersection& intersection, const PointLight& light, const Vec3& L) {
        auto shadowRay = Ray::createWithOffset(intersection.point, L, 100.);
        return isInShadow(intersection, light, shadowRay);
    };

    bool isInShadow(const Intersection& intersection, const PointLight& light, const Ray& shadowRay) {
        if (auto shadowIntersection = traceRayToClosestSceneIntersection(shadowRay)) {
            // if the shadow intersection is closer to the original intersection than the light, it's in shadow, otherwise the shadow intersection is behind the light, so it does not obscure it
            return (shadowIntersection->point - intersection.point).length() < (light.position - intersection.point).length() - 100 * epsilon;
        }
        return false;
    }

    /// shades a single intersection point
    /// outputs an un-tonemapped color, not for immediate display
    Vec3 shadeBlinnPhong(const Intersection& intersectionToShade, uint32_t bounces = 1, float_T currentIOR = 1.) {
        if (bounces > scene.nBounces)
            // could also return the background color, but black makes it easier to see when a render has too few bounces
            return Vec3(0.);

        const PhongMaterial& material = intersectionToShade.material->assumePhongMaterial();

        // material properties
        Vec3 diffuseColor = material.diffuseColorAtTextureCoords(intersectionToShade.textureCoords);
        // ambient intensity is global constant for all materials
        float_T ambientIntensity = PhongMaterial::ambientIntensity;
        // to match the provided images, use the diffuse color for the ambient color
        Vec3 ambient = diffuseColor * ambientIntensity;
        Vec3 specularColor = material.specularColor;
        float_T ks = material.ks;
        float_T specularExponentShinyness = material.specularExponent;

        auto calculateSpecularHighlights = [&]() -> Vec3 {
            Vec3 specularSum(0.);

            for(const auto& light: scene.pointLights) {
                // L: light vector
                Vec3 L = (light.position - intersectionToShade.point).normalized();
                if(isInShadow(intersectionToShade, light, L))
                    continue;

                // V: view vector (invert incoming ray so that it points outward)
                Vec3 V = -intersectionToShade.incomingRay->direction;
                // H: half vector half between L and V
                Vec3 H = (L + V).normalized();
                float_T specularWeight = std::pow(std::max(intersectionToShade.surfaceNormal.dot(H), (float_T) 0.), 
                        specularExponentShinyness);
                specularSum += specularColor * specularWeight * light.intensityPerColor * ks;
            }
            return specularSum;
        };

        // Handle transparent (refractive) materials differently
        if (material.refractiveIndex.has_value()) {
            // make sure to still have specular highlights on transparent objects
            // these would be weighted by the objects transmissiveness, but as that doesnt exist, just add them for now
            Vec3 finalColor = calculateSpecularHighlights();
            float_T materialIOR = *material.refractiveIndex;

            bool entering = intersectionToShade.incomingRay->direction.dot(intersectionToShade.surfaceNormal) < 0;
            Vec3 normal = entering ? intersectionToShade.surfaceNormal : -intersectionToShade.surfaceNormal;
            float_T etaRatio = entering ? currentIOR / materialIOR : materialIOR / currentIOR;

            float_T cosTheta_i = -normal.dot(intersectionToShade.incomingRay->direction);
            float_T sinTheta_t_squared = etaRatio * etaRatio * (1. - cosTheta_i * cosTheta_i);

            if (sinTheta_t_squared <= 1.) {
                float_T cosTheta_t = std::sqrt(1. - sinTheta_t_squared);
                Vec3 refractedDir = etaRatio * intersectionToShade.incomingRay->direction + 
                    (etaRatio * cosTheta_i - cosTheta_t) * normal;

                auto refractedRay = Ray::createWithOffset(intersectionToShade.point, refractedDir);
                // assume that the next material is air when we're leaving the object.
                // this means that all objects that are placed in side one another are actually hollow.
                // this simplifies things and works reasonably well for most cases
                float_T nextIOR = entering ? materialIOR : 1.;

                Vec3 refractedColor = scene.backgroundColor;
                if (auto refractedIntersection = traceRayToClosestSceneIntersection(refractedRay)) {
                    refractedColor = shadeBlinnPhong(*refractedIntersection, bounces + 1, nextIOR);
                }

                // tint the refraction by the diffuse color, to be able to make e.g. red glass
                // TODO could add something like a density parameter to the material, to decide how much to tint, but for now, thats just encoded in the diffuse color
                finalColor += refractedColor * diffuseColor;
                return finalColor;
            }else{
                // in this case we have *total internal reflection*
                // handle by simply going on with the normal lighting for now
                // This looks realistic if the material is reflective
                // (basically, the designer is responible for making the material look good, just like with the rest of phong shading)
            }

        }

        // Regular materials: ambient + diffuse + specular highlights + reflection
        Vec3 color(0.);

        // Ambient and diffuse
        color += ambient;  // ambient

        // diffuse
        float_T kd = material.kd;
        for(const auto& light: scene.pointLights) {
            Vec3 L = (light.position - intersectionToShade.point).normalized();
            if(isInShadow(intersectionToShade, light, L))
                continue;

            Vec3 N = intersectionToShade.surfaceNormal;
            float_T diffuseWeight = std::max(N.dot(L), (float_T) 0.);
            color += diffuseColor * diffuseWeight * light.intensityPerColor * kd;
        }

        // Add specular for all materials
        color += calculateSpecularHighlights();


        // Handle reflection if material is reflective
        // TODO could do fresnel here and for refractivity
        if(material.reflectivity.has_value()) {
            Vec3 reflectionDir = intersectionToShade.incomingRay->direction.reflect(intersectionToShade.surfaceNormal);
            auto reflectionRay = Ray::createWithOffset(intersectionToShade.point, reflectionDir);

            Vec3 reflectedColor = scene.backgroundColor;
            if (auto reflectionIntersection = traceRayToClosestSceneIntersection(reflectionRay)) {
                reflectedColor = shadeBlinnPhong(*reflectionIntersection, bounces + 1, currentIOR);
            }

            color = color.lerp(reflectedColor, *material.reflectivity);
        }

        return color;
    }

    /// performs local tone mapping, i.e. independent of the rest of the image
    /// this works nicely for the pathtracer, as it does not darken the entire image if a light is visible (like a simple global method would)
    /// it also looks good for 
    /// does *not* clamp the color yet, this is done in writing the buffer to the file
    Vec3 localLinearToneMapping(Vec3 color){
        // we cannot clamp the color here, because for incremental renders, we need the buffer to be in high-dynamic range,
        // i.e. if we clamped here, and then averaged with later samples, the image wouldn't be brightened up
        // (see PBR 3rd edition Figure 14.7 for more)
        return color * scene.camera->exposure * Camera::exposureCorrectionFactor;
    }

    /// UNUSED IN THE DEFAULT OPTIONS (enable via "tonemap: globalLinear")
    /// this can be used for phong shading, but it looks worse than the local tone mapping + clamping in my opinion.
    /// does *not* clamp the color yet, this is done in writing the buffer to the file
    Vec3 globalLinearToneMapping(Vec3 color, float_T maximumIntensity){
        return scene.camera->exposure * Camera::exposureCorrectionFactor * color / maximumIntensity;
    }

    /// only used for pathtraced shading, as the phong references don't seem to do gamma correction
    Vec3 gammaCorrect(Vec3 color){
        return Vec3(std::pow(color.x, 1./2.2), std::pow(color.y, 1./2.2), std::pow(color.z, 1./2.2));
    }

    template<bool useBVH = true>
    std::optional<Intersection> traceRayToClosestSceneIntersection(const Ray& ray){
        if constexpr(useBVH){
            return bvh.intersect(ray, scene.objects);
        }else{
            auto closestIntersection = std::optional<Intersection>();
            for(auto& object: scene.objects)
                if(auto intersection = object.intersect(ray))
                    if(!closestIntersection.has_value() || intersection->distance() < closestIntersection->distance())
                        closestIntersection = *intersection;

            return closestIntersection;
        }
    }

    template<RenderMode mode>
    void render(){
        std::println(stderr, "BVH num nodes: {} (bvh memory usage: ~{:.4f} MB)", bvh.numNodes(), bvh.numNodes() * sizeof(BVHNode) * 1e-6);
        std::println(stderr, "num objects: {}", bvh.objectRange.size());

        if constexpr(mode == RenderMode::DEBUG_BVH)
            renderDebugBVHToBuffer();
        else if constexpr(mode == RenderMode::DEBUG_NORMALS)
            renderDebugNormalsToBuffer();
        else if constexpr(mode == RenderMode::PATHTRACE || mode == RenderMode::PATHTRACE_INCREMENTAL){
            // until user closes stdin (Ctrl+D)
            if constexpr(mode == RenderMode::PATHTRACE_INCREMENTAL)
                std::println(stderr, "Rendering incrementally. Press Ctrl+D to stop after next frame");

            // === stop when the user closes stdin or presses Ctrl+C, skip to next '===' comment ===

            volatile std::atomic<bool> stopRendering = false;
            static_assert(std::atomic<bool>::is_always_lock_free);
            if(mode == RenderMode::PATHTRACE)
                stopRendering = true;

            // start a thread that stops the program when the user closes stdin
            std::jthread stopThread([&stopRendering]{
                char _[2];
                while(!stopRendering && std::fgets(_, sizeof(_), stdin) != nullptr);

                stopRendering = true;
                std::println(stderr, "Will stop rendering after this frame");
            });

            // if the user presses Ctrl+C, abort rendering
            auto signalHandler = [](int){
                // c functions due to signal-safety (7)
                char buf[] = "Received SIGINT, aborting rendering (might result in broken image)\n";
                write(STDERR_FILENO, buf, sizeof(buf));
                _exit(1);
            };
            std::signal(SIGINT, signalHandler);

            // === actual rendering ===

            // always render once
            renderPathtraceToBuffer();
            writeBufferToFile();

            // keep track of rendered state to be able to average the results (no fancy sampling for incremental yet)
            auto samplesPerPixel = scene.pathtracingSamples.samplesPerPixel;
            auto samplesPerPixelSoFar = samplesPerPixel;
            std::vector<Vec3> previousBuffer = hdrPixelBuffer;

            while (!stopRendering) {
                std::println(stderr, "Frame rendered, {} samples per pixel so far", samplesPerPixelSoFar);

                renderPathtraceToBuffer();
                const auto samplePixelsNow = samplesPerPixel + samplesPerPixelSoFar;
                // average the results
                for(auto [pixel, previousPixel]: std::ranges::views::zip(hdrPixelBuffer, previousBuffer)){
                    pixel = samplesPerPixel*pixel/samplePixelsNow + samplesPerPixelSoFar*previousPixel/samplePixelsNow;
                }
                writeBufferToFile();
                previousBuffer = hdrPixelBuffer;

                samplesPerPixelSoFar = samplePixelsNow;
            } 

            std::println(stderr, "Rendering stopped: Final image has {} samples per pixel", samplesPerPixelSoFar);
        }else{
#pragma omp parallel for
            for(uint32_t y = 0; y < scene.camera->heightPixels; y++){
                for(uint32_t x = 0; x < scene.camera->widthPixels; x++){
                    Ray cameraRay = scene.camera->generateRay(Vec2(x, y));

                    auto closestIntersection = traceRayToClosestSceneIntersection(cameraRay);

                    Vec3 pixelColor = scene.backgroundColor;
                    if(closestIntersection.has_value()){
                        if constexpr (mode == RenderMode::BINARY){
                            pixelColor = Vec3(1.0);
                        }else if constexpr (mode == RenderMode::PHONG){
                            pixelColor = shadeBlinnPhong(*closestIntersection);
                        }else{
                            static_assert(false, "Invalid render mode");
                        }
                    }

                    if(scene.toneMapMode == ToneMapMode::LOCAL_LINEAR)
                        bufferSpecificPixel(Vec2(x, y), localLinearToneMapping(pixelColor));
                    else if(scene.toneMapMode == ToneMapMode::GLOBAL_LINEAR)
                        // buffer raw color, tone map afterwards
                        bufferSpecificPixel(Vec2(x, y), pixelColor);
                    else
                        assert(false && "Invalid tone map mode");
                }
            }

            if(scene.toneMapMode == ToneMapMode::GLOBAL_LINEAR){
                // compute max brightness sequentially afterwards
                // if we comptued this while the initial image was rendered, the synchronization overhead would be tremendous
                float_T maxIntensity = 0.;
                for(auto& pixel: hdrPixelBuffer)
                    if(pixel.length() > maxIntensity)
                        maxIntensity = pixel.length();

                // apply global tone mapping
                for(auto& pixel: hdrPixelBuffer)
                    pixel = globalLinearToneMapping(pixel, maxIntensity);
            }
        }

        writeBufferToFile();
    }

    void renderDebugBVHToBuffer() {
        // serially, so we dont have to use atomic accesses on the max intensity
        float_T maxIntensity = 0.;
        for(uint32_t y = 0; y < scene.camera->heightPixels; y++){
            for(uint32_t x = 0; x < scene.camera->widthPixels; x++){
                Ray cameraRay = scene.camera->generateRay(Vec2(x, y));

                std::vector<std::pair<BoundingBox, int>> intersected_boxes;
                bvh.recursivelyCollectIntersectedBoxes(cameraRay, intersected_boxes);

                // just write the size to the buffer for now, and keep track of the max
                float_T intensity = intersected_boxes.size();
                maxIntensity = std::max(maxIntensity, intensity);
                bufferSpecificPixel(Vec2(x, y), Vec3(intensity, intensity, intensity));
            }
        }

        // then go through all of them again and normalize them, mark areas close to max intensity as red
        static constexpr float_T redThreshhold = 0.9;
        for(auto& pixel: hdrPixelBuffer){
            float_T intensity = pixel.x;
            if(intensity > redThreshhold * maxIntensity)
                pixel = Vec3(intensity, 0.0, 0.0);
            else
                pixel = pixel / maxIntensity;
        }
    }

    void renderDebugNormalsToBuffer() {
        for(uint32_t y = 0; y < scene.camera->heightPixels; y++){
            for(uint32_t x = 0; x < scene.camera->widthPixels; x++){
                Ray cameraRay = scene.camera->generateRay(Vec2(x, y));

                if(auto closestIntersection = traceRayToClosestSceneIntersection(cameraRay)){
                    auto normalBetweenZeroOne = closestIntersection->surfaceNormal * 0.5 + Vec3(0.5);
                    bufferSpecificPixel(Vec2(x, y), normalBetweenZeroOne);
                }
            }
        }
    }

    /*
     TODO not used right now - either throw away, or put into separate lambertian material
    enum ImportanceSamplingTechnique{
        UNIFORM,
        COSINE_WEIGHTED_HEMISPHERE,
    };

    template<ImportanceSamplingTechnique technique>
    Vec3 sampleHemisphere(const Vec3& normal){
        if constexpr (technique == COSINE_WEIGHTED_HEMISPHERE){
            // cosine weighted hemisphere sampling, to eliminate the dot product from the rendering equation
            // basically the approximation of the integral already divides by cos(theta), so the multiplying by theta
            // in the normal rendering equation gets cancelled out.
            // To get the correct value of dividing by the PDF (cos(theta)/pi), we have to multiply by pi again

            // Generate two random numbers for disk sampling
            float_T r = sqrt(randomFloat());
            float_T theta = 2.0 * PI * randomFloat();

            // Convert uniform disk samples to hemisphere samples
            float_T x = r * cos(theta);
            float_T y = r * sin(theta);

            // Project up to hemisphere
            float_T z = sqrt(1.0 - x*x - y*y);

            // Create a coordinate system from the normal
            // TODO that z check is strange
            Vec3 up = (std::abs(normal.z) < (1 - 100 * epsilon)) ? Vec3(0, 0, 1) : Vec3(1, 0, 0);
            Vec3 tangent = up.cross(normal).normalized();
            Vec3 bitangent = normal.cross(tangent);

            // Transform the local hemisphere direction to world space
            // TODO hmmm, this will always give positive results though, right?
            return (tangent * x + bitangent * y + normal * z).normalized();
        }else if constexpr (technique == UNIFORM){
            float_T phi = 2.0 * PI * randomFloat();
            float_T z = randomFloat();
            float_T r = std::sqrt(1.0 - z*z);

            // Create basis vectors
            Vec3 up = normal;
            Vec3 right(1, 0, 0);
            if (std::abs(up.y) < std::abs(up.x)) {
                right = Vec3(0, 1, 0);
            }

            Vec3 tangent = up.cross(right);
            Vec3 bitangent = up.cross(tangent);

            Vec3 sample = tangent * (r * std::cos(phi)) + 
                bitangent * (r * std::sin(phi)) + 
                up * z;
            assert(sample == sample.normalized() && "Sample must be on the unit hemisphere");

            return sample;
        } else {
            static_assert(false, "Invalid importance sampling technique");
        }
    }
    */

    Vec3 shadePathtraced(const Intersection& intersection, uint32_t bounces = 1){
        if(bounces > scene.nBounces)
            return Vec3(scene.backgroundColor);

        const PrincipledBRDFMaterial& material = intersection.material->assumePrincipledBRDFMaterial();

        // Start with emission
        Vec3 overallColor = material.emissionColor(intersection.textureCoords);

        // Create orthonormal basis for BRDF sampling
        Vec3 N = intersection.surfaceNormal.normalized();
        Vec3 V = (-intersection.incomingRay->direction).normalized();

        // TODO make this into a helper method somewhere, this code is duplicated in so many places
        // Create tangent space basis vectors
        auto [X, Y] = Vec3::createOrthonormalBasis(N);
        

        // TODO reintroduce comments from previous version

        unsigned actualSamplesTaken = 0;

        // Sample contribution for each hemisphere sample
        Vec3 accumulatedContributions(0.);
        for(unsigned hemisphereSampleNum = 0; 
                hemisphereSampleNum < scene.pathtracingSamples.hemisphereSamplesPerBounce; 
                hemisphereSampleNum++) {

            // Get sample from BRDF
            auto sample = material.sampleBRDF(V, N, X, Y);

            // Skip invalid samples
            if (sample.pdf <= epsilon) {
                continue;
            }

            // in this case, we're going to actually use the sample
            actualSamplesTaken++;

            // Create and trace ray in sampled direction
            auto incomingRay = Ray::createWithOffset(intersection.point, sample.direction);

            Vec3 incomingColor = scene.backgroundColor;
            if(auto incomingIntersection = traceRayToClosestSceneIntersection(incomingRay)) {
                incomingColor = shadePathtraced(*incomingIntersection, bounces + 1);
            }

            // Evaluate BRDF for this direction
            Vec3 brdfValue = material.evaluateBRDF(
                    intersection.textureCoords,
                    V,
                    sample.direction,
                    X, Y, N
                    );

            float_T cosTheta = std::max(0.0f, N.dot(sample.direction));

            // Accumulate contribution
            // NOTE: cosine term is included in BRDF evaluation
            accumulatedContributions += incomingColor * brdfValue * cosTheta/sample.pdf;
        }

        if(actualSamplesTaken > 0)
            // Average the samples
            overallColor += accumulatedContributions / actualSamplesTaken;

        if(!scene.pointLights.empty()){
            // sample point lights explicitly, because they are infinitessimally small, they can never be hit by a random ray
            // luckily, the pdf of the dirac delta distribution representing these cancels out with the light intensity of the point light itself, so we can simply add it, if the light is not in shadow
            // we could just sample all point lights for every bounce, but thats a bit wasteful again for the later bounces
            // -> randomly pick one, then compensate for that choice by multiplying with the number of point lights
            const auto& light = scene.randomPointLight();

            // optionally sample each light source multiple times
            // not strictly necessary because of monte carlo - we're sampling each pixel multiple times anyway
            // but this gives greater control, although it should be 1 in most cases
            Vec3 accumulatedContributions(0.);

            for(unsigned lightSampleNum = 0; lightSampleNum < scene.pathtracingSamples.pointLightsamplesPerBounce; lightSampleNum++){
                // permute the origin randomly if the light has some amount of softness
                Vec3 intersectionOriginPlusJitter = intersection.point + Vec3(randomFloat() - 0.5, randomFloat() - 0.5, randomFloat() - 0.5) * light.shadowSoftness;

                // TODO hmm, this would be the more accurate version, because we dont want to just sample in an even sphere around the intersection point, just on the surface
                // this does reduce noise, but it introduces strange artifacts for triangle meshes
                //                    Vec3 tangent(0.), bitangent(0.);
                //if (std::abs(intersection.surfaceNormal.x) < std::abs(intersection.surfaceNormal.y)) {
                //    tangent = Vec3(0, intersection.surfaceNormal.z, -intersection.surfaceNormal.y).normalized();
                //} else {
                //    tangent = Vec3(intersection.surfaceNormal.z, 0, -intersection.surfaceNormal.x).normalized();
                //}
                //bitangent = intersection.surfaceNormal.cross(tangent);
                //
                //// Generate random offset in tangent space
                //float r = sqrt(randomFloat()) * light.shadowSoftness; // sqrt for uniform disk sampling
                //float theta = randomFloat() * 2 * PI;
                //float x = r * cos(theta);
                //float y = r * sin(theta);
                //
                //// Apply offset in tangent space
                //Vec3 intersectionOriginPlusJitter = intersection.point +
                //(tangent * x + bitangent * y);

                Vec3 L = (light.position - intersectionOriginPlusJitter).normalized();

                if(isInShadow(intersection, light, L))
                    continue;

                // TODO I think this is the wrong way around for point lights

                Vec3 brdf = material.evaluateBRDF(intersection.textureCoords, V, L, X, Y, N);
                // weight the contribution by the angle between the normal and the light ray
                float_T weight = std::max(intersection.surfaceNormal.dot(L), (float_T) 0.);
                accumulatedContributions += brdf * weight * light.intensityPerColor;
            }

            // compensate for only sampling one light
            accumulatedContributions *= scene.pointLights.size();
            // compensate for the number of samples
            accumulatedContributions /= scene.pathtracingSamples.pointLightsamplesPerBounce;

            overallColor += accumulatedContributions;
        }

        return overallColor;
    }

    void renderPathtraceToBuffer(){
        // dynamic thread scheduling improves performance by ~10% on cornell box with 6 bounces (because some rays terminate early by bouncing into nothing)
#pragma omp parallel for collapse(2) schedule(dynamic, 64)
        for(uint32_t y = 0; y < scene.camera->heightPixels; y++){
            for(uint32_t x = 0; x < scene.camera->widthPixels; x++){
                const Vec2 pixelOrigin = Vec2(x, y);
                Vec3 colorSum = Vec3(0.);
                for(uint32_t sample = 0; sample < scene.pathtracingSamples.samplesPerPixel; sample++){
                    // permute ray randomly/evenly for jittered sampling
                    Vec2 permutedPixel = pixelOrigin + Vec2(randomFloat() - 0.5, randomFloat() - 0.5);

                    Ray cameraRay = scene.camera->generateRay(permutedPixel);

                    if(auto intersection = traceRayToClosestSceneIntersection(cameraRay)){
                        colorSum += shadePathtraced(*intersection);
                    }else{
                        // background color
                        colorSum += scene.backgroundColor;
                    }
                }

                bufferSpecificPixel(Vec2(x, y), localLinearToneMapping(gammaCorrect(colorSum / scene.pathtracingSamples.samplesPerPixel)));
            }
        }
    }

};
