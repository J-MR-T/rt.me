#pragma once
#include <cassert>
#include <cmath>
#include <memory>
#include <random>
#include <ranges>
#include <vector>
#include <optional>
#include <cstdint>
#include <string>
#include <fstream>
#include <print>
#include <atomic>
#include <thread>
#include <format>

// single precision for now, ~15% faster than double, but double precision is an option for maximum accuracy
using float_T = float;

const float_T epsilon = 1e-6;

inline bool implies(bool a, bool b){
    return !a || b;
}

/// generate a uniformly distributed random float in [0, 1)
inline float_T randomFloat() {
    // initial generated by chatgpt, thread safe by me
    // make thread safe by ensuring each thread has its own generator
    thread_local static std::random_device rd;
    thread_local static std::mt19937 gen(rd());
    thread_local static std::uniform_real_distribution<float_T> dis(0., 1.);
    return dis(gen);
}


struct Vec2{
    float_T x,y;

    // see Vec3 for details on these operators

    Vec2 operator+(const Vec2& other) const{
        return Vec2(x+other.x, y+other.y);
    }

    Vec2 operator-(const Vec2& other) const{
        return Vec2(x-other.x, y-other.y);
    }

    Vec2 operator*(float_T scalar) const{
        return Vec2(x*scalar, y*scalar);
    }

    Vec2 operator/(float_T scalar) const{
        return Vec2(x/scalar, y/scalar);
    }

    Vec2 operator*(const Vec2& other) const{
        return Vec2(x*other.x, y*other.y);
    }

    Vec2 operator/(const Vec2& other) const{
        return Vec2(x/other.x, y/other.y);
    }

    friend Vec2 operator*(float_T scalar, const Vec2& vec) {
        return vec * scalar;
    }

    float_T dot(const Vec2& other) const{
        return x*other.x + y*other.y;
    }

    float_T length() const{
        return std::sqrt(dot(*this));
    }

    Vec2 normalized() const{
        return *this / length();
    }
};

struct Vec3{
    float_T x,y,z;

    Vec3(float_T x, float_T y, float_T z) : x(x), y(y), z(z){ }

    explicit Vec3(float_T uniform) : x(uniform), y(uniform), z(uniform){ }

    Vec3() : Vec3(0.) {}

    Vec3 operator+(const Vec3& other) const{
        return Vec3(x+other.x, y+other.y, z+other.z);
    }

    Vec3 operator-(const Vec3& other) const{
        return Vec3(x-other.x, y-other.y, z-other.z);
    }

    // scalar multiplication/division
    Vec3 operator*(float_T scalar) const{
        return Vec3(x*scalar, y*scalar, z*scalar);
    }

    // Friend function to overload for scalar * vector
    friend Vec3 operator*(float_T scalar, const Vec3& vec) {
        return vec * scalar;
    }
    friend Vec3 operator/(float_T scalar, const Vec3& vec) {
        return vec / scalar;
    }

    Vec3 operator/(float_T scalar) const{
        return Vec3(x/scalar, y/scalar, z/scalar);
    }

    // elementwise multiplication/division
    Vec3 operator*(Vec3 other) const{
        return Vec3(x*other.x, y*other.y, z*other.z);
    }

    Vec3 operator/(Vec3 other) const{
        return Vec3(x/other.x, y/other.y, z/other.z);
    }

    bool operator==(const Vec3& other) const{
        // epsilon comparison
        return std::abs(x - other.x) < epsilon &&
            std::abs(y - other.y) < epsilon &&
            std::abs(z - other.z) < epsilon;
    }

    float_T operator[](size_t index) const{
        assert(index < 3 && "Index out of bounds");
        return index == 0 ? x : (index == 1 ? y : z);
    }

    Vec3 operator-() const {
        return *this * -1;
    }

    // state modifying operators
    Vec3& operator+=(const Vec3& other){
        x += other.x;
        y += other.y;
        z += other.z;
        return *this;
    }
    Vec3& operator*=(float_T scalar){
        x *= scalar;
        y *= scalar;
        z *= scalar;
        return *this;
    }
    Vec3& operator/=(float_T scalar){
        x /= scalar;
        y /= scalar;
        z /= scalar;
        return *this;
    }

    float_T dot(const Vec3& other) const{
        return x*other.x + y*other.y + z*other.z;
    }

    float_T length() const{
        return std::sqrt(dot(*this));
    }

    Vec3 normalized() const{
        return *this / length();
    }

    Vec3 cross(const Vec3& other) const {
        return Vec3(
                 y*other.z - z*other.y,
                 z*other.x - x*other.z,
                 x*other.y - y*other.x
               );
    }

    Vec3 clamp(float_T min, float_T max){
        return Vec3(
            std::clamp(x, min, max),
            std::clamp(y, min, max),
            std::clamp(z, min, max)
        );
    }

    Vec3 lerp(const Vec3& other, float_T t) const{
        return *this * (1-t) + other * t;
    }

    Vec3 min(const Vec3& other) const{
        return Vec3(
            std::min(x, other.x),
            std::min(y, other.y),
            std::min(z, other.z)
        );
    }

    Vec3 max(const Vec3& other) const{
        return Vec3(
            std::max(x, other.x),
            std::max(y, other.y),
            std::max(z, other.z)
        );
    }

    float_T distance(const Vec3& other) const{
        return (*this - other).length();
    }

    Vec3 reflect(const Vec3& normal) const{
        const Vec3 incomingDirection = *this;
        return incomingDirection - normal * 2 * (incomingDirection.dot(normal));
    }
};

// overload std::format/std::println for Vec3
template <>
struct std::formatter<Vec3> : std::formatter<std::string> {
  auto format(const Vec3& v, format_context& ctx) const {
    return formatter<string>::format(std::format("[{}, {}, {}]", v.x, v.y, v.z), ctx);
  }
};

template <>
struct std::basic_format_arg<Vec3> {

};

struct PPMWriter{
private:
    std::ofstream file;

    std::ofstream::pos_type pixelDataStart;


public:
    std::string filePath;
    uint32_t width, height;

    PPMWriter(std::string_view filePath, uint32_t width, uint32_t height)
        : filePath(filePath), width(width), height(height){
        file = std::ofstream(this->filePath, std::ios::binary);
        if(file.fail()){
            std::perror("Couldn't open file for writing");
            std::exit(EXIT_FAILURE);
        }

        // we're writing binary ppm, i.e. P6

        // write header
        file << "P6\n" << width << " " << height << "\n255\n";

        // the rest is the pixel data, which we'll write later
        this->pixelDataStart = file.tellp();
    }

    /// write a single pixel in binary format, pixels are iterated over row by row
    void writePixel(uint8_t r, uint8_t g, uint8_t b){
        file.put(r);
        file.put(g);
        file.put(b);
    }

    void writePixel(Vec3 color){
        assert(color == color.clamp(0,1) && "Color must be in the range [0,1]");
        writePixel(
            static_cast<uint8_t>(color.x * 255),
            static_cast<uint8_t>(color.y * 255),
            static_cast<uint8_t>(color.z * 255)
        );
    }

    void rewind(){
        file.flush();
        file.seekp(pixelDataStart);
    }
};


struct Ray{
    Vec3 origin;
    Vec3 direction;
    Vec3 invDirection;

    // TODO maybe make some kind of "prevent self intersection" constructor to dedupliate some code

    /// assumes that the direction is normalized!
    Ray(Vec3 origin, Vec3 direction, Vec3 invDirection)
        : origin(origin), direction(direction), invDirection(invDirection){
        assert(direction == direction.normalized() && "Ray direction must be normalized");
        assert(invDirection == 1./direction && "Ray inverse direction must be the reciprocal of the direction");
    }

    Ray(Vec3 origin, Vec3 direction) : Ray(origin, direction, 1./direction){}

};

// TODO could use CRTP later, but normal dynamic dispatch is fine for now

struct Camera{
    // TODO think about these
    Vec3 position;
    Vec3 direction;
    Vec3 down; // we're using a right-handed coordinate system, as PPM has (0,0) in the top left, so we want to go down not up
    Vec3 right;
    float_T width;
    uint64_t widthPixels;
    float_T height;
    uint64_t heightPixels;
    float_T exposure; // TODO use

    virtual ~Camera() = default; 

    // TODO maybe experiment with && and std::move to avoid some copies
    Camera(Vec3 position,
           Vec3 lookAt,
           Vec3 up,
           float_T width,
           float_T height,
           float_T exposure) : position(position), direction((lookAt - position).normalized()), down(-(up.normalized())), right(direction.cross(down).normalized()), width(width), widthPixels(std::round(width)), height(height), heightPixels(std::round(height)), exposure(exposure){
        const float_T aspectRatio = width / height;
        imagePlaneDimensions = Vec2(aspectRatio*imagePlaneHeight, imagePlaneHeight);
    }

    /// gets a pixel in pixel screen space, i.e. [0,width]x[0,height]
    /// outputs a ray in world space, i.e. adjusting for the camera's position
    virtual Ray generateRay(Vec2 pixelInScreenSpace) const = 0;

protected:
    float_T imagePlaneHeight = 1.0;
    Vec2 imagePlaneDimensions;

    /// gets a pixel in pixel screen space, i.e. [0,width]x[0,height]
    Vec3 pixelInWorldSpace(Vec2 pixelInScreenSpace) const {
        // so, the pixel is in the range [0,width]x[0,height]
        // we want to map this to [-0.5,0.5]x[-0.5,0.5] in the camera's space
        // which is then mapped to the image plane in world space

        constexpr float_T pixelWidthHeight = 1.0;

        Vec2 pixelCenterInCameraSpace = Vec2(
            (pixelInScreenSpace.x + /* use center of pixel */ pixelWidthHeight/2) / width - 0.5,
            (pixelInScreenSpace.y + pixelWidthHeight/2) / height - 0.5
        );


        // scale to world space scale (no translation yet) by multiplying by the image plane dimensions
        Vec2 pixelScaledByWorldSpace = Vec2(pixelCenterInCameraSpace.x * imagePlaneDimensions.x, pixelCenterInCameraSpace.y * imagePlaneDimensions.y);

        // now after scaling to world space, translate to world space (i.e. the cameras position), and add the camera's right/up directions
        Vec3 pixelOrigin = position + right*pixelScaledByWorldSpace.x + down*pixelScaledByWorldSpace.y;
        return pixelOrigin;
    }

    // TODO something about this is off I think, the image seems a little bit stretched
    // TODO I think part of it is the assumed 1 unit distance to the image plane
    void setImagePlaneDimensionsFromFOV(float_T fovDegrees){
        const float_T verticalFOVRad = fovDegrees * (M_PI / 180.0); // Convert FOV to radians
        imagePlaneHeight = 2. * tan(verticalFOVRad / 2.); // Distance to image plane is 1 unit
        const float_T aspectRatio = width / height;
        imagePlaneDimensions = Vec2(imagePlaneHeight * aspectRatio, imagePlaneHeight);
    }
};

struct OrthographicCamera : public Camera{

    OrthographicCamera(Vec3 position, Vec3 lookAt, Vec3 up, float_T width, float_T height, float_T exposure)
        : Camera(position, lookAt, up, width, height, exposure){ }

    virtual Ray generateRay(Vec2 pixelInScreenSpace) const override{
        // for an orthographic camera, basically just shoot a ray in the look direction, through the pixel center
        return Ray(
            pixelInWorldSpace(pixelInScreenSpace),
            direction
        );
    }
};

struct PinholePerspectiveCamera : public Camera{
    PinholePerspectiveCamera(
        Vec3 position,
        Vec3 lookAt,
        Vec3 up,
        float_T fovDegrees,
        float_T width,
        float_T height,
        float_T exposure)
        : Camera(position, lookAt, up, width, height, exposure) {
        // Calculate image plane height based on FOV and set image plane dimensions
        setImagePlaneDimensionsFromFOV(fovDegrees);
    }

    /// gets a pixel in pixel screen space, i.e. [0,width]x[0,height]
    /// outputs a ray in world space, i.e. adjusting for the camera's position
    /// Generates a ray from the camera position through the specified pixel
    virtual Ray generateRay(Vec2 pixelInScreenSpace) const override {
        // Use pixelInWorldSpace to get the point on the image plane in world space
        Vec3 pointOnImagePlane = pixelInWorldSpace(pixelInScreenSpace) + /* place image plane 1 unit away from camera */ direction;

        // Calculate ray direction from camera position to point on image plane
        Vec3 rayDirection = (pointOnImagePlane - position).normalized();

        return Ray(position, rayDirection);
    }
};

/// thin lens camera with depth of field
/// aperture does not affect the amound of light let in, only the depth of field,
/// as its easier to adjust the amount of light via the exposure
struct SimplifiedThinLensCamera : public Camera {
    float_T focalLength;
    float_T apertureRadius;
    // Distance to focal plane
    float_T focusDistance; 

    /// takes focal length in mm, focal distance in meters
    SimplifiedThinLensCamera(
        Vec3 position,
        Vec3 direction,
        Vec3 up,
        float_T fovDegrees,
        float_T width,
        float_T height,
        float_T exposure,
        float_T fStop,
        float_T focalLength,       // in mm
        float_T focusDistance)     // in meters
        : Camera(position, direction, up, width, height, exposure),
          focalLength(0.001 * focalLength), // convert mm to meter
          apertureRadius((0.001 * focalLength / fStop) /* = diameter, so halve it to get radius */ * 0.5),
          focusDistance(focusDistance) {
        // TODO for a proper (i.e. not simplified) thin lens camera, should use the focal length to calculate the image plane dimensions, and place it behind the lens
        setImagePlaneDimensionsFromFOV(fovDegrees);
    }

    Vec2 sampleAperture() const {
        float_T theta = 2.0 * M_PI * randomFloat();
        float_T r = sqrt(randomFloat()) * apertureRadius;
        // Convert from local space to world space using image plane dimensions per pixel
        // for the purposes of the aperture, we need to assume the image plane has its "correct" dimensions, i.e. scale it down by the focal length
        // TODO explain 0.001 factor
        return Vec2(r * cos(theta), r * sin(theta)) * 1000. * focalLength;
    }

    virtual Ray generateRay(Vec2 pixelInScreenSpace) const override {
        // generated by ai

        // Get the point on the image plane in world space
        // TODO for a proper (i.e. not simplified) thin lens camera, should use the focal length as distance to image plane, not just 1 (and place it behind the lens)
        Vec3 pointOnImagePlane = pixelInWorldSpace(pixelInScreenSpace) + direction;

        // Calculate the direction from the camera position to the point on the image plane
        Vec3 rayDirection = (pointOnImagePlane - position).normalized();

        // Sample a point on the lens aperture
        Vec2 lensSample = sampleAperture();
        Vec3 lensPoint = position + right * lensSample.x + down * lensSample.y;

        // Calculate the point on the focal plane
        float_T t = focusDistance / rayDirection.dot(direction);
        Vec3 focalPoint = position + rayDirection * t;

        // Calculate the new ray direction from the lens point to the focal point
        Vec3 newRayDirection = (focalPoint - lensPoint).normalized();

        return Ray(lensPoint, newRayDirection);
    }
};

struct Color8Bit{
    uint8_t r,g,b;
};

struct Texture{
    uint32_t width, height;
    // dont use vectors, would waste memory
    // TODO check vs performance benefit of not having to convert to Vec3 at runtime
    std::vector<Color8Bit> pixels;

    Texture(uint32_t width, uint32_t height, std::vector<Color8Bit> pixels)
        : width(width), height(height), pixels(pixels){
        assert(width * height == pixels.size() && "Texture dimensions don't match pixel count");
    }

    /// initialize an empty texture to be filled later
    Texture(uint32_t width, uint32_t height)
        : width(width), height(height), pixels(width*height){ }

    void fillUninitialized(std::vector<Color8Bit>&& pixels){
        assert(width * height == pixels.size() && "Texture dimensions don't match pixel count");
        this->pixels = std::move(pixels);
    }

    Vec3 colorAt(const Vec2& textureCoords) const{
        // wrap around
        float_T x = std::fmod(textureCoords.x, 1.);
        float_T y = std::fmod(textureCoords.y, 1.);

        // scale to pixel space
        uint32_t pixelX = static_cast<uint32_t>(x * width);
        uint32_t pixelY = static_cast<uint32_t>(y * height);

        auto pixel = pixels[pixelY * width + pixelX];
        return Vec3(pixel.r / 255., pixel.g / 255., pixel.b / 255.);
    }
};

struct MaterialBase{
    Vec3 diffuseBaseColor;
    // share textures to reduce memory usage
    std::optional<std::shared_ptr<Texture>> texture;

    /// if the material has a texture, get the diffuse color at the given texture coordinates,
    /// otherwise just return the diffuse color
    /// The texture coordinates here need to be interpolated from the vertices of e.g. the triangle
    Vec3 diffuseColorAtTextureCoords(const Vec2& textureCoords) const {
        if(texture.has_value())
            return (*texture)->colorAt(textureCoords);
        else
            return diffuseBaseColor;
    }
};

struct PhongMaterial : MaterialBase {
    Vec3 specularColor;
    float_T ks,kd;
    uint64_t specularExponent;
    std::optional<float_T> reflectivity;
    std::optional<float_T> refractiveIndex;

    PhongMaterial(
            Vec3 diffuseColor,
            Vec3 specularColor,
            float_T ks,
            float_T kd,
            uint64_t specularExponent,
            std::optional<float_T> reflectivity,
            std::optional<float_T> refractiveIndex,
            std::optional<std::shared_ptr<Texture>> texture)
        : MaterialBase(diffuseColor, texture), specularColor(specularColor), ks(ks), kd(kd), specularExponent(specularExponent), reflectivity(reflectivity), refractiveIndex(refractiveIndex) { }

    /// if the material has a texture, get the diffuse color at the given texture coordinates,
    /// otherwise just return the diffuse color
    /// The texture coordinates here need to be interpolated from the vertices of e.g. the triangle
    Vec3 diffuseColorAtTextureCoords(const Vec2& textureCoords) const {
        if(texture.has_value())
            return (*texture)->colorAt(textureCoords);
        else
            return diffuseBaseColor;
    }

};

struct PrincipledBRDFMaterial : MaterialBase{
    float_T emissiveness;

    float_T baseColorDiffuseIntensity;
    float_T metallic;
    float_T subsurface;
    float_T specular;
    float_T roughness;
    float_T specularTint;
    float_T anisotropic;
    float_T sheen;
    float_T sheenTint;
    float_T clearcoat;
    float_T clearcoatGloss;

    // TODO sheen went missing along the way

    PrincipledBRDFMaterial(
            Vec3 diffuseColor,
            std::optional<std::shared_ptr<Texture>> texture,
            float_T emissiveness,
            float_T baseColorIntensity,
            float_T metallic,
            float_T subsurface,
            float_T specular,
            float_T roughness,
            float_T specularTint,
            float_T anisotropic,
            float_T sheen,
            float_T sheenTint,
            float_T clearcoat,
            float_T clearcoatGloss)
        : MaterialBase(diffuseColor, texture), emissiveness(emissiveness), baseColorDiffuseIntensity(baseColorIntensity), metallic(metallic), subsurface(subsurface), specular(specular), roughness(roughness), specularTint(specularTint), anisotropic(anisotropic), sheen(sheen), sheenTint(sheenTint), clearcoat(clearcoat), clearcoatGloss(clearcoatGloss) { }

    Vec3 emissionColor(const Vec2& textureCoords) const {
        return emissiveness * diffuseColorAtTextureCoords(textureCoords);
    }

    
private:
    // Utility functions
    static float_T sqr(float_T x) { return x * x; }
    static float_T safe_sqrt(float_T x) { return std::sqrt(std::max(epsilon, x)); }
    static float_T luminance(const Vec3& color) {
        return std::max(epsilon, color.dot(Vec3(0.3f, 0.6f, 0.1f)));
    }

    // Distribution functions
    static float_T GTR1(float_T NdotH, float_T a) {
        if (a >= 1.0f) return (float_T)(1.0 / M_PI);
        float_T a2 = std::max(epsilon, a * a);
        float_T t = 1.0 + (a2 - 1.0) * NdotH * NdotH;
        return (a2 - 1.0f) / (M_PI * std::log(a2) * t);
    }

    // Helper for anisotropic calculations
    static void calculateAnisotropicParams(float_T roughness, float_T anisotropic,
                                         float_T& ax, float_T& ay) {
        roughness = std::max(0.001f, roughness);
        
        // Modify how anisotropic affects the aspect ratio
        // Map anisotropic [0,1] to a more useful range for aspect ratio
        float_T t = anisotropic * 0.9f;  // Keep maximum anisotropy slightly below 1
        ax = std::max(0.001f, roughness * (1.0f + t));
        ay = std::max(0.001f, roughness * (1.0f - t));
    }

    Vec3 sampleGGXVNDF(const Vec3& V, float_T roughness, const Vec3& X, const Vec3& Y, const Vec3& N) const {
        float_T u1 = randomFloat();
        float_T u2 = randomFloat();

        // Calculate anisotropic roughness
        float_T ax, ay;
        calculateAnisotropicParams(roughness, anisotropic, ax, ay);

        // Transform V to tangent space
        Vec3 Vt = Vec3(V.dot(X), V.dot(Y), V.dot(N));

        // Sample visible normal distribution
        float_T phi = 2.0f * M_PI * u1;
        float_T theta = std::atan(roughness * std::sqrt(u2 / (1.0f - u2)));

        float_T cos_theta = std::cos(theta);
        float_T sin_theta = std::sin(theta);
        float_T cos_phi = std::cos(phi);
        float_T sin_phi = std::sin(phi);

        // Compute half vector in tangent space
        Vec3 Hlocal = Vec3(
            sin_theta * cos_phi,
            sin_theta * sin_phi,
            cos_theta
        ).normalized();

        // Unstretch
        Hlocal = Vec3(Hlocal.x * ax, Hlocal.y * ay, Hlocal.z).normalized();

        // Transform back to world space
        return (X * Hlocal.x + Y * Hlocal.y + N * Hlocal.z).normalized();
    }

    // GGX (Trowbridge-Reitz) Distribution Function
    static float_T D_GGX_aniso(const Vec3& H, const Vec3& N,
                              const Vec3& X, const Vec3& Y,
                              float_T ax, float_T ay) {
        float_T NdotH = std::max(epsilon, N.dot(H));

        // Early exit if normal and half vector are perpendicular
        if (NdotH <= 0) return 0;

        // Project H onto the tangent plane
        float_T HdotX = H.dot(X);
        float_T HdotY = H.dot(Y);

        // Calculate the squared slopes
        float_T ax2 = sqr(ax);
        float_T ay2 = sqr(ay);

        // Calculate the normalization factor
        float_T denom = (sqr(HdotX) / ax2 + sqr(HdotY) / ay2 + sqr(NdotH));

        return 1.0f / (M_PI * ax * ay * sqr(denom));
    }


    // TODO aniso does not really work yet

    // Geometric shadowing functions
    //static float_T smithG_GGX_aniso(float_T NdotV, float_T VdotX, float_T VdotY, float_T ax, float_T ay) {
    //    return 1.0f / (NdotV + safe_sqrt(sqr(VdotX*ax) + sqr(VdotY*ay) + sqr(NdotV)));
    //}

    static float_T smithG_GGX_aniso(float_T NdotV, float_T VdotX, float_T VdotY, 
            float_T NdotL, float_T LdotX, float_T LdotY,
            float_T ax, float_T ay) {
        float_T lambda_V = NdotV + safe_sqrt(sqr(VdotX*ax) + sqr(VdotY*ay) + sqr(NdotV));
        float_T lambda_L = NdotL + safe_sqrt(sqr(LdotX*ax) + sqr(LdotY*ay) + sqr(NdotL));

        return 2. / (lambda_V * lambda_L);
    }

    static float_T smithG(float_T NdotV, float_T alphaG) {
        float_T a = alphaG * alphaG;
        float_T b = NdotV * NdotV;
        return 1. / (NdotV + safe_sqrt(a + b - a * b));
    }

    // TODO probably make the eval and caculate pdf functions into lambdas
    // TODO clean up all of the clamps and epsilons

    // BRDF component evaluation functions
    Vec3 evaluateDiffuseBRDF(const Vec3& baseColor, float_T NdotL, float_T NdotV, float_T LdotH) const {
        float_T FL = std::pow(1.0f - NdotL, 5.0f);
        float_T FV = std::pow(1.0f - NdotV, 5.0f);
        float_T Rr = 2.0f * roughness * sqr(LdotH);
        
        float_T Fd90 = 0.5f + 2.0f * roughness * sqr(LdotH);
        float_T Fd = std::lerp(1.0f, Fd90, FL) * std::lerp(1.0f, Fd90, FV);
        
        float_T Fss90 = Rr;
        float_T Fss = std::lerp(1.0f, Fss90, FL) * std::lerp(1.0f, Fss90, FV);
        float_T ss = 1.25f * (Fss * (1.0f / (NdotL + NdotV) - 0.5f) + 0.5f);
        
        return (1.0f / M_PI) * std::lerp(Fd, ss, subsurface) * baseColor * baseColorDiffuseIntensity * (1.0f - metallic);
    }

    Vec3 evaluateSpecularBRDF(const Vec3& baseColor, const Vec3& V, const Vec3& L, const Vec3& H,
            const Vec3& X, const Vec3& Y, const Vec3& N) const {
        float_T NdotL = std::max(epsilon, N.dot(L));
        float_T NdotV = std::max(epsilon, N.dot(V));
        float_T NdotH = std::max(epsilon, N.dot(H));
        float_T LdotH = std::max(epsilon, L.dot(H));
        
        // Calculate anisotropic roughness parameters
        float_T ax, ay;
        calculateAnisotropicParams(roughness, anisotropic, ax, ay);
        
        // Calculate tint color
        Vec3 tint = (luminance(baseColor) > epsilon) ? 
            baseColor / luminance(baseColor) : Vec3(1.0f);
        
        // Calculate specular color with proper metallic workflow
        Vec3 specularColor = (Vec3(1.0f) * 0.08f * specular)
                                .lerp(tint, specularTint)
                                .lerp(baseColor, metallic);
        
        // Fresnel term (Schlick's approximation)
        Vec3 F = specularColor + (Vec3(1.0f) - specularColor) * std::pow(1.0f - LdotH, 5.0f);
        
        // Distribution term (GGX/Trowbridge-Reitz)
        float_T D = D_GGX_aniso(H, N, X, Y, ax, ay);
        
        // Geometric term
        float_T G = smithG_GGX_aniso(NdotV, V.dot(X), V.dot(Y),
                                    NdotL, L.dot(X), L.dot(Y),
                                    ax, ay);
        
        return F * D * G;
    }

    Vec3 evaluateClearcoatBRDF(const Vec3& V, const Vec3& L, const Vec3& H, const Vec3& N) const {
        float_T NdotL = std::max(epsilon, N.dot(L));
        float_T NdotV = std::max(epsilon, N.dot(V));
        float_T NdotH = std::max(epsilon, N.dot(H));
        float_T LdotH = std::max(epsilon, L.dot(H));
        
        
    // Fixed IOR of 1.5 gives F0 of 0.04
    const float_T F0 = 0.04f;
    // Use the same Fresnel equation as specular but with fixed F0
    float_T Fr = F0 + (1.0f - F0) * std::pow(1.0f - LdotH, 5.0f);
    
    // You can either keep GTR1 or switch to GGX for consistency
    // Option 1: Keep GTR1 with better gloss mapping
    float_T alpha = std::lerp(0.1f, 0.001f, clearcoatGloss);
    float_T Dr = GTR1(NdotH, alpha);
    
    // Option 2: Use GGX (like specular) with fixed roughness
    // float_T roughness = std::lerp(0.25f, 0.1f, clearcoatGloss);
    // float_T Dr = D_GGX_aniso(H, N, X, Y, roughness, roughness);
    
    // Use the same geometry term as specular but with fixed roughness
    const float_T fixed_roughness = 0.25f;
    float_T Gr = smithG(NdotL, fixed_roughness) * smithG(NdotV, fixed_roughness);
    
    // No color tinting - clearcoat is always white
    return Vec3(clearcoat * Gr * Fr * Dr);
}


    // TODO probably calculate the weights during construction

    // Sampling strategy weights
    float_T calculateDiffuseWeight() const {
        // Diffuse weight only depends on:
        // - metallic (metals don't have diffuse)
        // - clearcoat (reduces underlying diffuse)
        // - and the diffuse intensity itself
        return (1.0f - metallic) * baseColorDiffuseIntensity * (1.0f - 0.5f * clearcoat * clearcoatGloss);
    }

    float_T calculateSpecularWeight() const {
        // Specular weight depends on:
        // - metallic (increases specular)
        // - roughness (decreases specular more aggressively)
        // - specular parameter
        return (1.0f + metallic) * specular * (1.0f - roughness * roughness);
    }

    float_T calculateClearcoatWeight() const {
        return clearcoat * clearcoatGloss;
    }

    float_T calculateDiffusePDF(const Vec3& L, const Vec3& N) const {
        // For cosine-weighted hemisphere sampling, the PDF is cos(theta)/pi
        // where theta is the angle between the normal and sampled direction
        float_T cosTheta = std::max(epsilon, N.dot(L));
        return cosTheta / M_PI;
    }

    float_T calculateSpecularPDF(const Vec3& V, const Vec3& L, const Vec3& H,
            const Vec3& N, const Vec3& X, const Vec3& Y,
            float_T roughness) const {
        float_T NdotH = std::max(epsilon, N.dot(H));
        float_T VdotH = std::max(epsilon, V.dot(H));

        // Calculate anisotropic roughness parameters
        float_T ax, ay;
        calculateAnisotropicParams(roughness, anisotropic, ax, ay);

        // Calculate the GGX distribution term
        float_T D = D_GGX_aniso(H, N, X, Y, ax, ay);

        // The PDF for GGX importance sampling is:
        // pdf = D * NdotH / (4 * VdotH)
        // This comes from the Jacobian of the half-vector transformation

        // Handle numerical stability for grazing angles
        if (VdotH < epsilon || NdotH < epsilon) {
            return 0.0f;
        }

        float_T pdf = (D * NdotH) / (4.0f * VdotH);

        // Use less aggressive clamping - allow for higher values at sharp specular peaks
        return std::clamp(pdf, epsilon, 1e8f);
    }

    float_T calculateClearcoatPDF(const Vec3& V, const Vec3& L, const Vec3& H,
            const Vec3& N) const {
        float_T NdotH = std::max(epsilon, N.dot(H));
        float_T VdotH = std::max(epsilon, V.dot(H));

        // Clearcoat uses GTR1 distribution with fixed roughness
        // interpolated based on clearcoatGloss
        float_T alpha = std::lerp(0.1f, 0.001f, clearcoatGloss);

        // D_GTR1 term
        float_T D = GTR1(NdotH, alpha);

        // Same Jacobian as specular
        float_T jacobian = 1.0f / (4.0f * VdotH);

        if (VdotH < epsilon || NdotH < epsilon) {
            return 0.0f;
        }

        float_T pdf = D * NdotH * jacobian;
        return std::clamp(pdf, epsilon, 1e3f);
    }

public:
    struct BRDFSample {
        Vec3 direction;  // Sampled direction in world space
        float_T pdf;    // Probability density of the sample
    };

    // Main sampling function
    BRDFSample sampleBRDF(const Vec3& V, 
                         const Vec3& N, const Vec3& X, const Vec3& Y) const {
         // Calculate weights
        float_T diffuseWeight = calculateDiffuseWeight();
        float_T specularWeight = calculateSpecularWeight();
        float_T clearcoatWeight = calculateClearcoatWeight();
        
        // Normalize weights
        float_T totalWeight = diffuseWeight + specularWeight + clearcoatWeight;
        diffuseWeight /= totalWeight;
        specularWeight /= totalWeight;
        clearcoatWeight /= totalWeight;

        BRDFSample sample;
        float_T rand = randomFloat();
        
        // Sample direction based on weights
        if (rand < diffuseWeight) {
            // Cosine-weighted hemisphere sampling
            float_T r1 = randomFloat();
            float_T r2 = randomFloat();
            float_T phi = 2.0f * M_PI * r1;
            float_T cosTheta = std::sqrt(r2);
            float_T sinTheta = std::sqrt(1.0f - r2);
            
            sample.direction = (
                X * (std::cos(phi) * sinTheta) +
                Y * (std::sin(phi) * sinTheta) +
                N * cosTheta
            ).normalized();
        }
        else if (rand < diffuseWeight + specularWeight) {
            Vec3 H = sampleGGXVNDF(V, roughness * roughness, X, Y, N);
            sample.direction = (-V).reflect(H);
        }
        else {
            Vec3 H = sampleGGXVNDF(V, 0.25f, X, Y, N);
            sample.direction = (-V).reflect(H);
        }

        // Calculate PDF for each strategy
        float_T diffusePdf = calculateDiffusePDF(sample.direction, N);
        float_T specularPdf = calculateSpecularPDF(V, sample.direction, (V + sample.direction).normalized(), N, X, Y, roughness);
        float_T clearcoatPdf = calculateClearcoatPDF(V, sample.direction, (V + sample.direction).normalized(), N);

        // Combine PDFs using balance heuristic
        sample.pdf = diffuseWeight * diffusePdf + 
                    specularWeight * specularPdf + 
                    clearcoatWeight * clearcoatPdf;

        return sample;
    }

    // TODO rename params (also for sample)

    // Combined BRDF evaluation
    Vec3 evaluateBRDF(const Vec2& textureCoords, const Vec3& V, const Vec3& L,
                      const Vec3& X, const Vec3& Y, const Vec3& N) const {
        if (N.dot(L) <= 0.0f || N.dot(V) <= 0.0f) {
            return Vec3(0.0f);
        }
        
        Vec3 H = (L + V).normalized();
        Vec3 baseColor = diffuseColorAtTextureCoords(textureCoords);
        
        float_T NdotL = std::max(epsilon, N.dot(L));
        float_T NdotV = std::max(epsilon, N.dot(V));
        float_T LdotH = std::max(epsilon, L.dot(H));
        
        return (
            evaluateDiffuseBRDF(baseColor, NdotL, NdotV, LdotH) +
            evaluateSpecularBRDF(baseColor, V, L, H, X, Y, N) +
            evaluateClearcoatBRDF(V, L, H, N)
        );
    }
};

const PhongMaterial defaultPhongMaterial = PhongMaterial(Vec3(1,1,1), Vec3(1,1,1), 0.5, 0.5, 32, 0.0, 0.0, std::nullopt);
const PrincipledBRDFMaterial defaultPrincipledBRDFMaterial = PrincipledBRDFMaterial(Vec3(1.), std::nullopt, 0., 1., 0., 0., 0.5, 1., 0., 1., 0., 0., 0., 0.);

struct Material {
    std::variant<PhongMaterial, PrincipledBRDFMaterial> variant;

    const PhongMaterial& assumePhongMaterial() const {
        assert(std::holds_alternative<PhongMaterial>(variant) && "Material is not a Phong material");
        return std::get<PhongMaterial>(variant);
    }

    const PrincipledBRDFMaterial& assumePrincipledBRDFMaterial() const {
        assert(std::holds_alternative<PrincipledBRDFMaterial>(variant) && "Material is not a BRDF material");
        return std::get<PrincipledBRDFMaterial>(variant);
    }

    bool hasTexture() const {
        return std::visit([&](auto&& object){
            return object.texture.has_value();
        }, variant);
    }
};

struct Intersection{
    // ray that caused the intersection
    const Ray* incomingRay;
    // TODO instead of storing the intersection point, could store the distance along the ray
    Vec3 point;
    Vec3 surfaceNormal;
    const Material* material;
    Vec2 textureCoords;

    Intersection(const Ray* incomingray, Vec3 point, Vec3 surfaceNormal, const Material* material, Vec2 textureCoords)
        : incomingRay(incomingray), point(point), surfaceNormal(surfaceNormal), material(material), textureCoords(textureCoords){
        assert(surfaceNormal == surfaceNormal.normalized() && "Surface normal must be normalized");
        assert(incomingRay && "Incoming ray must not be null");
        assert(material && "Material must not be null");
    }

    float_T distance() const{
        return (point - incomingRay->origin).length();
    }

};

struct Sphere {
    Vec3 center;
    float_T radius;
    Material material;

    Sphere(Vec3 center, float_T radius, Material material)
        : center(center), radius(radius), material(std::move(material)){ }

    std::optional<Intersection> intersect(const Ray& ray) const {
        // mostly generated by chatgpt

        // Vector from the ray's origin to the sphere's center
        Vec3 oc = ray.origin - center;

        // Coefficients of the quadratic equation (a*t^2 + b*t + c = 0)
        float_T a = ray.direction.dot(ray.direction);               // a = D•D
        float_T b = 2.0 * oc.dot(ray.direction);                    // b = 2 * oc•D
        float_T c = oc.dot(oc) - radius * radius;                   // c = (oc•oc - r^2)

        // Discriminant of the quadratic equation
        float_T discriminant = b * b - 4 * a * c;

        // No intersection if the discriminant is negative
        if (discriminant < 0) {
            return std::nullopt;
        }

        // Calculate the two intersection distances along the ray
        float_T sqrtDiscriminant = std::sqrt(discriminant);
        float_T t1 = (-b - sqrtDiscriminant) / (2.0 * a);
        float_T t2 = (-b + sqrtDiscriminant) / (2.0 * a);


        // Choose the closest intersection point in front of the ray origin
        float_T t = (t1 > 0) ? t1 : ((t2 > 0) ? t2 : -1);
        // if both are behind the ray, return no intersection
        if (t < 0) {
            return std::nullopt;
        }

        // Calculate intersection details
        Vec3 intersectionPoint = ray.origin + ray.direction * t;
        Vec3 intersectionNormal = (intersectionPoint - center).normalized();

        // Calculate texture coordinates
        Vec3 localPoint = (intersectionPoint - center) / radius; // Normalize to unit sphere

        // Calculate spherical coordinates
        float_T theta = std::atan2(localPoint.z, localPoint.x); // Longitude
        float_T phi = std::acos(localPoint.y);                  // Latitude

        // Map spherical coordinates to texture coordinates (u, v)
        float_T u = (theta + M_PI) / (2 * M_PI);
        float_T v = phi / M_PI;

        return Intersection(&ray, std::move(intersectionPoint), std::move(intersectionNormal), &material, Vec2(u, v));
    }

    /// equality (explicitly do not check for material, because two spheres with the exact same position and geometry should not have different materials; this is the same for all other shapes)
    bool operator==(const Sphere& other) const {
        return center == other.center && radius == other.radius;
    }
};

struct Cylinder {
    Vec3 center;
    float_T radius;
    float_T eachSideHeight;
    Vec3 axis;
    Material material;

    Cylinder(Vec3 center, float_T radius, float_T height, Vec3 axis, Material material)
        : center(center), radius(radius), eachSideHeight(height), axis(axis), material(std::move(material)){ }

    std::optional<Intersection> intersect(const Ray& ray) const {
        // mostly generated by chatgpt

        Vec3 d = ray.direction - axis * ray.direction.dot(axis);  // Projected ray direction onto the cylinder's plane
        Vec3 oc = ray.origin - center;
        Vec3 oc_proj = oc - axis * oc.dot(axis);                  // Projected ray origin onto the cylinder's plane

        float_T a = d.dot(d);
        float_T b = 2. * d.dot(oc_proj);
        float_T c = oc_proj.dot(oc_proj) - radius * radius;
        std::optional<Intersection> closestIntersection = std::nullopt;

        // Quadratic discriminant for side wall intersection
        float_T discriminant = b * b - 4 * a * c;
        if (discriminant >= 0) {
            float_T sqrtDiscriminant = std::sqrt(discriminant);
            for (float_T t : { (-b - sqrtDiscriminant) / (2.0 * a), (-b + sqrtDiscriminant) / (2.0 * a) }) {
                if (t < 0) continue;

                Vec3 point = ray.origin + ray.direction * t;
                Vec3 localPoint = point - center;
                float_T projectionOnAxis = localPoint.dot(axis);

                // Check if intersection point is within height limits of the cylinder
                if (projectionOnAxis >= -eachSideHeight && projectionOnAxis <= eachSideHeight) {
                    Vec3 normal = (localPoint - axis * projectionOnAxis).normalized();
                    Intersection intersection(&ray, point, normal, &material, textCoordsOfSideIntersection(point));


                    // Update closest intersection
                    if (!closestIntersection || t < (closestIntersection->point - ray.origin).length()) {
                        closestIntersection = intersection;
                    }
                }
            }
        }

        auto checkCapIntersection = [&](const Vec3& capCenter, const Vec3& capNormal) -> std::optional<Intersection> {
            float_T denom = ray.direction.dot(capNormal);
            if (std::abs(denom) < 1e-6) return std::nullopt;

            float_T tCap = (capCenter - ray.origin).dot(capNormal) / denom;
            if (tCap < 0) return std::nullopt;

            Vec3 point = ray.origin + ray.direction * tCap;
            if ((point - capCenter).length() <= radius) {  // Check if within radius of cap
                Intersection intersection(&ray, point, capNormal, &material, textCoordsOfCapIntersection(point));
                return intersection;
            }
            return std::nullopt;
        };

        // Check intersections with the base and top caps
        for (auto& cap : { std::make_pair(center - axis * eachSideHeight, -axis), 
                std::make_pair(center + axis * eachSideHeight, axis) }) {
            if (auto capIntersection = checkCapIntersection(cap.first, cap.second); capIntersection) {
                float_T capDistance = (capIntersection->point - ray.origin).length();
                if (!closestIntersection || capDistance < (closestIntersection->point - ray.origin).length()) {
                    closestIntersection = capIntersection;
                }
            }
        }

        return closestIntersection;
    }

    bool operator==(const Cylinder& other) const {
        return center == other.center && radius == other.radius && eachSideHeight == other.eachSideHeight && axis == other.axis;
    }

private:
    Vec2 textCoordsOfSideIntersection(const Vec3& intersectionPoint) const {
        Vec3 baseToIntersection = intersectionPoint - (center - axis * eachSideHeight);
        float_T vPosAlongAxis = baseToIntersection.dot(axis);        
        float_T v = vPosAlongAxis / (2 * eachSideHeight);  // Map height position to v in [0, 1]

        Vec3 circumferentialDir = (baseToIntersection - axis * vPosAlongAxis).normalized();
        float_T theta = std::atan2(circumferentialDir.z, circumferentialDir.x);        
        float_T u = (theta + M_PI) / (2 * M_PI);  // Map angle to u in [0, 1]

        return Vec2(u,v);
    }

    Vec2 textCoordsOfCapIntersection(const Vec3& intersectionPoint) const {
        // Determine which cap (top or bottom) we're on based on axis direction and height
        Vec3 capCenter = intersectionPoint.dot(axis) > 0 ? (center + axis * eachSideHeight) : (center - axis * eachSideHeight);
        Vec3 localCapPoint = intersectionPoint - capCenter;

        // Map `localCapPoint` to polar coordinates within the cap radius
        float_T r = localCapPoint.length() / radius;  // Distance from center mapped to [0, 1]
        float_T capTheta = std::atan2(localCapPoint.z, localCapPoint.x);

        float_T u = 0.5 + r * std::cos(capTheta) / 2;  // Map radial distance and angle to texture u
        float_T v = 0.5 + r * std::sin(capTheta) / 2;  // Map radial distance and angle to texture v

        return Vec2(u,v);
    }
};

struct Triangle {
    Vec3 v0,v1,v2;
    // TODO could try deduplicating materials for triangle objects later on, for big meshes that all have the same material
    Material material;
    // these are only valid if the material has a texture
    Vec2 texCoordv0, texCoordv1, texCoordv2;

    // TODO could precompute bounding box or mins/maxes for faster building of the BVH

    Vec3 normal = (v1-v0).cross(v2-v0).normalized();

    Triangle(Vec3 v0, Vec3 v1, Vec3 v2, Material material, Vec2 texCoordv0, Vec2 texCoordv1, Vec2 texCoordv2)
        : v0(v0), v1(v1), v2(v2), material(std::move(material)), texCoordv0(texCoordv0), texCoordv1(texCoordv1), texCoordv2(texCoordv2){ }

    Vec3 faceNormal() const{
        // TODO in the future, could interpolate this with the rest of the mesh
        return normal;
    }

    // TODO normal vector interpolation at any point for smooth shading (requires knowledge of the rest of the mesh)

    std::optional<Intersection> intersect(const Ray& ray) const {
        // Möller–Trumbore intersection
        // mostly generated by chatgpt
        Vec3 edge1 = v1 - v0;
        Vec3 edge2 = v2 - v0;
        Vec3 h = ray.direction.cross(edge2);
        float_T a = edge1.dot(h);

        // If a is near zero, the ray is parallel to the triangle
        if (std::abs(a) < epsilon) return std::nullopt;

        float_T f = 1.0 / a;
        Vec3 s = ray.origin - v0;
        float_T u = f * s.dot(h);

        // Check if the intersection is outside the triangle
        if (u < 0.0 || u > 1.0) return std::nullopt;

        Vec3 q = s.cross(edge1);
        float_T v = f * ray.direction.dot(q);

        // Check if the intersection is outside the triangle
        if (v < 0.0 || u + v > 1.0) return std::nullopt;

        // Calculate the distance along the ray to the intersection point
        float_T t = f * edge2.dot(q);

        // Only accept intersections that are in front of the ray origin
        if (t > epsilon) {
            Vec3 intersectionPoint = ray.origin + ray.direction * t;
            Vec3 normal = faceNormal();  // Use the constant normal for the triangle

            // Ensure the normal points against the ray's direction,
            // we want to make sure that backfaces look like frontfaces
            // TODO I think this makes stuff righter, in particular shadow calculations for flipped normals
            if (normal.dot(ray.direction) > 0) {
                normal = -normal;
            }

            // interpolate texture coordinates
            // calculate barycentric coordinate `w`
            float_T w = 1. - u - v;

            // interpolate the texture coordinates using barycentric weights
            Vec2 interpolatedTexCoord = texCoordv0 * w + texCoordv1 * u + texCoordv2 * v;

            return Intersection(&ray, intersectionPoint, normal, &material, interpolatedTexCoord);
        }

        return std::nullopt;
    }
    
    bool operator==(const Triangle& other) const{
        return v0 == other.v0 && v1 == other.v1 && v2 == other.v2;
    }
};

struct SceneObject {
    // use a variant to store different types of objects, to avoid virtual function calls (expensive)
    std::variant<Triangle, Sphere, Cylinder> variant;

    std::optional<Intersection> intersect(const Ray& ray) const {
        return std::visit([&](auto&& object){
            return object.intersect(ray);
        }, variant);
    }

    bool operator==(const SceneObject& other) const{
        return variant == other.variant;
    }
};

struct BoundingBox{
    Vec3 min, max;

    BoundingBox() :
          min(Vec3(std::numeric_limits<float_T>::max()))
        , max(Vec3(-std::numeric_limits<float_T>::max())) 
    { }

    /// assumes min and max are actually <= each other, componentwise
    BoundingBox(Vec3 min, Vec3 max)
        : min(min), max(max){
        assert(min.x <= max.x && min.y <= max.y && min.z <= max.z && "Trying to construct invalid bounding box");
    }

    // TODO look at the min/max 0 things, there has to be a better way
    explicit BoundingBox(SceneObject object): min(0.), max(0.){
        std::visit([this](auto&& object){
            using T = std::decay_t<decltype(object)>;
            if constexpr(std::is_same_v<T, Triangle>){
                min = Vec3(
                    std::min({object.v0.x, object.v1.x, object.v2.x}),
                    std::min({object.v0.y, object.v1.y, object.v2.y}),
                    std::min({object.v0.z, object.v1.z, object.v2.z})
                );
                max = Vec3(
                    std::max({object.v0.x, object.v1.x, object.v2.x}),
                    std::max({object.v0.y, object.v1.y, object.v2.y}),
                    std::max({object.v0.z, object.v1.z, object.v2.z})
                );
            }else if constexpr(std::is_same_v<T, Sphere>){
                min = object.center - Vec3(object.radius);
                max = object.center + Vec3(object.radius);
            }else if constexpr(std::is_same_v<T, Cylinder>){
                // cylinder can be encompassed in a box with one corner at one side of the bottom cap, and the other at the other side of the top cap
                const Vec3 bottomCapCenter = object.center - object.axis * object.eachSideHeight;
                const Vec3 topCapCenter = object.center + object.axis * object.eachSideHeight;

                // we'll be shifting points along the 2 "cap" axes (by the radius), so mask out the "height" axis of the cylinder
                const Vec3 axisMask = Vec3(1.) - object.axis;
                // and then invert the axis for one corner, and use it directly for the other
                const Vec3 bottomCapCorner = bottomCapCenter - axisMask * Vec3(object.radius);
                const Vec3 topCapOppositeCorner = topCapCenter + axisMask * Vec3(object.radius);

                min = bottomCapCorner;
                max = topCapOppositeCorner;
            }else{
                static_assert(false, "Unexpected object type");
            }
        }, object.variant);
        assert(min.x <= max.x && min.y <= max.y && min.z <= max.z && "Internal error: invalid bounding box constructed");
    }

    Vec3 center() const { 
        return (min + max) * 0.5; 
    }

    Vec3 extent() const { 
        return max - min; 
    }

    BoundingBox merge(const BoundingBox& other) const {
        return BoundingBox(
            min.min(other.min),
            max.max(other.max)
        );
    }

    bool contains(const Vec3& point) const {
        return point.x >= min.x && point.x <= max.x &&
               point.y >= min.y && point.y <= max.y &&
               point.z >= min.z && point.z <= max.z;
    }

    /*
       TODO probably either remove or merge this with the intersects function
    float_T intersection_distance(const Ray& ray) const {
        Vec3 invDir = Vec3(1.) / ray.direction;

        Vec3 t0 = (min - ray.origin) * invDir;
        Vec3 t1 = (max - ray.origin) * invDir;

        Vec3 tmin = t0.min(t1);
        Vec3 tmax = t0.max(t1);

        float_T tenter = std::max(std::max(tmin.x, tmin.y), tmin.z);
        float_T texit = std::min(std::min(tmax.x, tmax.y), tmax.z);

        return tenter <= texit && texit >= 0 ? tenter : std::numeric_limits<float_T>::max();
    }
    */

    bool intersects(const Ray& ray) const {
        Vec3 invDir = Vec3(1.) / ray.direction;
        
        Vec3 t0 = (min - ray.origin) * invDir;
        Vec3 t1 = (max - ray.origin) * invDir;
        
        Vec3 tmin = t0.min(t1);
        Vec3 tmax = t0.max(t1);
        
        float_T tenter = std::max(std::max(tmin.x, tmin.y), tmin.z);
        float_T texit = std::min(std::min(tmax.x, tmax.y), tmax.z);
        
        return tenter <= texit && texit >= 0;
    }

    bool overlaps(const BoundingBox& other) const {
        return min.x <= other.max.x && max.x >= other.min.x &&
               min.y <= other.max.y && max.y >= other.min.y &&
               min.z <= other.max.z && max.z >= other.min.z;
    }

    float_T surface_area() const {
        Vec3 d = extent();
        return 2. * (d.x * d.y + d.y * d.z + d.z * d.x);
    }
};

/// bounding volume hierarchy
struct BVHNode{
    struct ObjectRange{
        /// left-inclusive, right-exclusive
        std::pair<size_t, size_t> objectRange;

        // allow implicit conversion
        ObjectRange(std::pair<size_t, size_t> objectRange)
            : objectRange(objectRange){ }

        ObjectRange(size_t first, size_t last)
            : objectRange(std::make_pair(first, last)){ }

        size_t size() const{
            return objectRange.second - objectRange.first;
        }

        bool empty() const{
            return objectRange.first == objectRange.second;
        }

        bool operator==(const ObjectRange& other) const{
            return objectRange == other.objectRange;
        }

        size_t begin() const{
            return objectRange.first;
        }

        size_t end() const{
            return objectRange.second;
        }

        // implicit conversion to pair
        operator std::pair<size_t, size_t>() const{
            return objectRange;
        }
    };

    BoundingBox bounds;
    std::unique_ptr<BVHNode> left, right;
    /// range of objects in the scene object list that this node represents
    ObjectRange objectRange;
    // -> from this, it is clear that objects can only overlap "linearly", i.e. only if they are adjacent in the list

    static constexpr size_t MAX_DEPTH = 16;
    // TODO maybe remove in future, or make an option; not in use currently, because even though it reduces memory usage, it doesn't improve performance
    //static constexpr size_t MIN_OBJECTS = 4;

public:
    /// REORDERS THE OBJECTS VECTOR
    /// but does not store it anywhere, the objects explicitly live outside the BVH
    BVHNode(ObjectRange objectRangeP, std::vector<SceneObject>& objects, uint32_t depth = 0) 
        : objectRange(std::move(objectRangeP))
    {
        assert(objectRange.begin() <= objectRange.end() && "Invalid objectRange");
        assert(objectRange.end() <= objects.size() && "objectRange exceeds object vector");

        // get the maximum bounds of all objects in the range
        bounds = boundsFromObjectRange(objectRange, objects);
        
        if (depth >= MAX_DEPTH)
            return;

        // Find the axis with greatest extent, to be able to subdivide as equally as possible
        Vec3 extent = bounds.extent();
        int splitAxis = 0;
        if (extent.y > extent.x) splitAxis = 1;
        if (extent.z > extent[splitAxis]) splitAxis = 2;

        // partition objects along the median of the split axis
        auto begin = objects.begin() + objectRange.begin();
        auto end = objects.begin() + objectRange.end();
        std::nth_element(begin, begin + (end - begin)/2, end,
            [splitAxis](const SceneObject& a, const SceneObject& b) {
                return computeCentroid(a)[splitAxis] < computeCentroid(b)[splitAxis];
            }
        );
        size_t midIndex = objectRange.begin() + (objectRange.end() - objectRange.begin()) / 2;
        
        // only create children if there are actually objects in the range
        if (midIndex > objectRange.begin() && midIndex < objectRange.end()) {
            left = std::make_unique<BVHNode>(ObjectRange(objectRange.begin(), midIndex), objects, depth + 1);
            right = std::make_unique<BVHNode>(ObjectRange(midIndex, objectRange.end()), objects, depth + 1);
        }
    }

    std::optional<Intersection> intersect(const Ray& ray, const std::vector<SceneObject>& objects) const {
        if (!bounds.intersects(ray))
            return std::nullopt;

        if (isLeaf()) {
            auto closestIntersection = std::optional<Intersection>();

            for (auto i = objectRange.begin(); i < objectRange.end(); ++i)
                if (auto intersection = objects[i].intersect(ray))
                    if(!closestIntersection.has_value() || intersection->distance() < closestIntersection->distance())
                        closestIntersection = *intersection;

            return closestIntersection;
        }

        // here, we're just checking both boxes
        // TODO but we could check the least number of nodes by:
        // a checking the closer box first
        // b only checking the other box if the boxes overlap, or if the closer box has no intersection

        auto leftIntersection = left->intersect(ray, objects);
        auto rightIntersection = right->intersect(ray, objects);
        if(leftIntersection.has_value() && rightIntersection.has_value())
            return leftIntersection->distance() < rightIntersection->distance() ? leftIntersection : rightIntersection;
        else if(leftIntersection.has_value())
            // don't std::move this, to allow copy elision
            return leftIntersection;
        else
            // if the right intersection is empty, this will also return nullopt
            return rightIntersection;
    }

    bool isLeaf() const { 
        return !left && !right; 
    }

    // === The following public methods are all for debugging purposes ===

    void verifyBVH(const std::vector<SceneObject>& objects) const {
#ifdef NDEBUG
        std::println(stderr, "verifyBVH should only be called in debug mode");
        std::abort();
#endif
        // Verify range validity
        assert(objectRange.begin() <= objectRange.end() && "Invalid objectRange");
        assert(objectRange.end() <= objects.size() && "objectRange exceeds vector size");

        if (!isLeaf()) {
            // Verify children exist
            // (this is a bit stupid, because isLeaf would prob fail first, but just in case)
            assert(left && right && "Non-leaf node missing children");

            // Recursively verify children
            left->verifyBVH(objects);
            right->verifyBVH(objects);

            // Verify child objectRanges overlap linearly, i.e.
        }
    }

    void recursivelyCollectIntersectedBoxes(const Ray& ray, std::vector<std::pair<BoundingBox, int>>& boxes, int depth = 0) const {
        if (bounds.intersects(ray)) {
            boxes.push_back({bounds, depth});
            
            if (!isLeaf()) {
                left->recursivelyCollectIntersectedBoxes(ray, boxes, depth + 1);
                right->recursivelyCollectIntersectedBoxes(ray, boxes, depth + 1);
            }
        }
    }

    uint64_t numNodes() const {
        if (isLeaf())
            return 1;
        return 1 + left->numNodes() + right->numNodes();
    }

private:
    static Vec3 computeCentroid(const SceneObject& object) {
        return std::visit([](const auto& obj) -> Vec3 {
            using T = std::decay_t<decltype(obj)>;
            if constexpr (std::is_same_v<T, Triangle>) {
                return (obj.v0 + obj.v1 + obj.v2) / 3.;
            } else if constexpr (std::is_same_v<T, Sphere>) {
                return obj.center;
            } else if constexpr (std::is_same_v<T, Cylinder>) {
                return obj.center;
            } else{
                static_assert(false, "Unexpected object type");
            }
        }, object.variant);
    }

    static BoundingBox boundsFromObjectRange(ObjectRange range, 
                                    const std::vector<SceneObject>& objects) {
        BoundingBox bounds;
        for (size_t i = range.begin(); i < range.end(); ++i) {
            bounds = bounds.merge(BoundingBox(objects[i]));
        }
        return bounds;
    }
};

struct PointLight{
    Vec3 position;
    // the json files seem to integrate intensity and color into one vector
    Vec3 intensityPerColor;
    /// 0 shadow softness means copletely hard shadows, higher means softer
    /// ONLY AFFECTS PATHTRACED SHADOWS
    float_T shadowSoftness;
};

enum class RenderMode{
    BINARY,
    PHONG,
    DEBUG_BVH,
    DEBUG_NORMALS,
    PATHTRACE,
    // continue rendering after rendering the first frame, and average the results
    PATHTRACE_INCREMENTAL,
};

struct Scene{
    uint32_t nBounces;
    RenderMode renderMode;
    std::unique_ptr<Camera> camera;
    Vec3 backgroundColor;

    std::vector<PointLight> pointLights;

    const PointLight& randomPointLight() const {
        assert(!pointLights.empty() && "No point lights in scene");
        // std::rand() % pointLights.size() literally triples render time, so use this instead
        return pointLights[randomFloat() * pointLights.size()];
    }

    std::vector<SceneObject> objects;

    struct {
        uint32_t samplesPerPixel;
        uint32_t apertureSamplesPerPixelSample;
        uint32_t pointLightsamplesPerBounce;
        /// be aware, that these are the samples that will result in exponentially more rays
        /// there is almost no reason to use this higher than 1, more samples per pixel is the better monte carlo answer to this
        uint32_t hemisphereSamplesPerBounce;
    } pathtracingSamples;

    Scene(uint32_t nBounces,
            RenderMode renderMode,
            std::unique_ptr<Camera> camera,
            Vec3 backgroundColor,
            std::vector<PointLight> lights,
            std::vector<SceneObject> objects,
            uint32_t pathtracingSamplesPerPixel,
            uint32_t pathtracingApertureSamplesPerPixelSample,
            uint32_t pathtracingPointLightsamplesPerBounce,
            uint32_t pathtracingHemisphereSamplesPerBounce)
        : nBounces(nBounces),
        renderMode(renderMode),
        camera(std::move(camera)),
        backgroundColor(backgroundColor),
        pointLights(std::move(lights)),
        objects(std::move(objects)),
        pathtracingSamples(
            pathtracingSamplesPerPixel,
            pathtracingApertureSamplesPerPixelSample,
            pathtracingPointLightsamplesPerBounce,
            pathtracingHemisphereSamplesPerBounce
        ){ }
};

struct Renderer{
    Scene scene;
    PPMWriter writer;
    // ordered the same way a PPM file is, row by row
    // use a buffer instead of writing to the file immediately to be able to do it in parallel
    std::vector<Vec3> hdrPixelBuffer;

    BVHNode bvh;

    Renderer(Scene&& scene, std::string_view outputFilePath)
        : scene(std::move(scene)),
          writer(outputFilePath, this->scene.camera->widthPixels, this->scene.camera->heightPixels),
          hdrPixelBuffer(this->scene.camera->widthPixels*this->scene.camera->heightPixels, Vec3(0.)),
          // this reorders the objects in the scene to be able to build the BVH
          bvh(BVHNode(BVHNode::ObjectRange(0, this->scene.objects.size()), this->scene.objects))
    {}

    void bufferSpecificPixel(Vec2 pixel, Vec3 color){
        assert(pixel.x >= 0 && pixel.x < scene.camera->width && pixel.y >= 0 && pixel.y < scene.camera->height && "Pixel out of range");
        assert(scene.camera->width * pixel.y + pixel.x < hdrPixelBuffer.size() && "Pixel out of range");
        hdrPixelBuffer[pixel.y * scene.camera->width + pixel.x] = color;
    }

    void writeBufferToFile(){
        writer.rewind();
        // write buffer to file
        assert(hdrPixelBuffer.size() == scene.camera->widthPixels * scene.camera->heightPixels && "Pixel buffer size mismatch");
        for(auto& hdrPixel: hdrPixelBuffer){
            auto ldrPixel = hdrPixel.clamp(0., 1.);
            writer.writePixel(ldrPixel);
        }
    }


    bool isInShadow(const Intersection& intersection, const PointLight& light, const Vec3& L) {
        Vec3 shadowRayOrigin = intersection.point + L * (100 * epsilon);
        Ray shadowRay(shadowRayOrigin, L);

        return isInShadow(intersection, light, shadowRay);
    };

    bool isInShadow(const Intersection& intersection, const PointLight& light, const Ray& shadowRay) {
        if (auto shadowIntersection = traceRayToClosestSceneIntersection(shadowRay)) {
            return (shadowIntersection->point - intersection.point).length() < (light.position - intersection.point).length() - 100 * epsilon;
        }
        return false;
    }

    /// shades a single intersection point
    /// outputs an un-tonemapped color, not for immediate display
    Vec3 shadeBlinnPhong(const Intersection& intersectionToShade, uint32_t bounces = 1, float_T currentIOR = 1.) {
        if (bounces > scene.nBounces)
            return Vec3(0.);

        const PhongMaterial& material = intersectionToShade.material->assumePhongMaterial();

        // material properties
        Vec3 diffuse = material.diffuseColorAtTextureCoords(intersectionToShade.textureCoords);
        float_T ambientIntensity = 0.25;
        Vec3 ambient = diffuse * ambientIntensity;
        Vec3 specular = material.specularColor;
        float_T ks = material.ks;
        float_T specularExponentShinyness = material.specularExponent;

        auto calculateSpecularHighlights = [&]() -> Vec3 {
            Vec3 specularSum(0.);

            for(const auto& light: scene.pointLights) {
                Vec3 L = (light.position - intersectionToShade.point).normalized();
                if(isInShadow(intersectionToShade, light, L))
                    continue;

                Vec3 V = -intersectionToShade.incomingRay->direction;
                Vec3 H = (L + V).normalized();
                float_T spec = std::pow(std::max(intersectionToShade.surfaceNormal.dot(H), (float_T) 0.), 
                        specularExponentShinyness);
                specularSum += specular * spec * light.intensityPerColor * ks;
            }
            return specularSum;
        };

        // Handle transparent (refractive) materials differently
        if (material.refractiveIndex.has_value()) {
            // make sure to still have specular highlights on transparent objects
            // these would be weighted by the objects transmissiveness, but as that doesnt exist, just add them for now
            Vec3 finalColor = calculateSpecularHighlights();
            float_T materialIOR = *material.refractiveIndex;

            bool entering = intersectionToShade.incomingRay->direction.dot(intersectionToShade.surfaceNormal) < 0;
            Vec3 normal = entering ? intersectionToShade.surfaceNormal : -intersectionToShade.surfaceNormal;
            float_T etaRatio = entering ? currentIOR / materialIOR : materialIOR / currentIOR;

            float_T cosTheta_i = -normal.dot(intersectionToShade.incomingRay->direction);
            float_T sinTheta_t_squared = etaRatio * etaRatio * (1. - cosTheta_i * cosTheta_i);

            if (sinTheta_t_squared <= 1.) {
                float_T cosTheta_t = std::sqrt(1. - sinTheta_t_squared);
                Vec3 refractedDir = etaRatio * intersectionToShade.incomingRay->direction + 
                    (etaRatio * cosTheta_i - cosTheta_t) * normal;

                Ray refractedRay(intersectionToShade.point + refractedDir * (10 * epsilon), refractedDir);
                // TODO refracting exiting being air doesnt really work I think, it should somehow be dependent on whether the intersection is inside the current intersected object or outside
                float_T nextIOR = entering ? materialIOR : 1.;

                Vec3 refractedColor = scene.backgroundColor;
                if (auto refractedIntersection = traceRayToClosestSceneIntersection(refractedRay)) {
                    refractedColor = shadeBlinnPhong(*refractedIntersection, bounces + 1, nextIOR);
                }

                // tint the refraction by the diffuse color, to be able to make e.g. red glass
                // TODO could add something like a density parameter to the material, to decide how much to tint, but for now, thats just encoded in the diffuse color
                finalColor += refractedColor * diffuse;
                return finalColor;
            }else{
                // Total internal reflection
                // handle by simply going on with the normal lighting for now
                // TODO to look somewhat realistic, this curently requires the material to be reflective
                //      but thats somewhat consistent with the phong lighting model idea: you're responsible for
                //      a realistic looking material, not the renderer
            }

        }

        // Regular materials: ambient + diffuse + specular + reflection
        Vec3 color(0.);

        // Ambient and diffuse
        color += ambient;  // ambient

        float_T kd = material.kd;
        for(const auto& light: scene.pointLights) {
            Vec3 L = (light.position - intersectionToShade.point).normalized();
            if(isInShadow(intersectionToShade, light, L))
                continue;

            Vec3 N = intersectionToShade.surfaceNormal;
            float_T diff = std::max(N.dot(L), (float_T) 0.);
            color += diffuse * diff * light.intensityPerColor * kd;  // diffuse
        }

        // Add specular for all materials
        color += calculateSpecularHighlights();

        // Handle reflection if material is reflective
        // TODO could do fresnel here and for refractivity
        if(material.reflectivity.has_value()) {
            Vec3 reflectedColor = scene.backgroundColor;
            Vec3 reflectionDir = intersectionToShade.incomingRay->direction.reflect(intersectionToShade.surfaceNormal);
            Ray reflectionRay(intersectionToShade.point + reflectionDir * (10 * epsilon), reflectionDir);

            if (auto reflectionIntersection = traceRayToClosestSceneIntersection(reflectionRay)) {
                reflectedColor = shadeBlinnPhong(*reflectionIntersection, bounces + 1, currentIOR);
            }

            color = color.lerp(reflectedColor, *material.reflectivity);
        }

        return color;
    }

    /// does *not* clamp the color, this is done in writing the pixel to the buffer
    Vec3 linearToneMapping(Vec3 color){
        return color*scene.camera->exposure * 15.;
    }

    Vec3 gammaCorrect(Vec3 color){
        return Vec3(std::pow(color.x, 1./2.2), std::pow(color.y, 1./2.2), std::pow(color.z, 1./2.2));
    }

    template<bool useBVH = true>
    std::optional<Intersection> traceRayToClosestSceneIntersection(const Ray& ray){
        if constexpr(useBVH){
            return bvh.intersect(ray, scene.objects);
        }else{
            auto closestIntersection = std::optional<Intersection>();
            for(auto& object: scene.objects)
                if(auto intersection = object.intersect(ray))
                    if(!closestIntersection.has_value() || intersection->distance() < closestIntersection->distance())
                        closestIntersection = *intersection;

            return closestIntersection;
        }
    }

    template<RenderMode mode>
    void render(){
        std::println(stderr, "BVH num nodes: {} (bvh memory usage: {} MB)", bvh.numNodes(), bvh.numNodes() * sizeof(BVHNode) * 1e-6);
        std::println(stderr, "num objects: {}", bvh.objectRange.size());

        if constexpr(mode == RenderMode::DEBUG_BVH)
            renderDebugBVHToBuffer();
        else if constexpr(mode == RenderMode::DEBUG_NORMALS)
            renderDebugNormalsToBuffer();
        else if constexpr(mode == RenderMode::PATHTRACE || mode == RenderMode::PATHTRACE_INCREMENTAL){
            // until user closes stdin (Ctrl+D)
            if constexpr(mode == RenderMode::PATHTRACE_INCREMENTAL)
                std::println(stderr, "Rendering incrementally. Press Ctrl+D to stop after next render");

            // TODO incremental and immediate still don't seem to have the exact same results for the same sample number

            std::atomic<bool> stopRendering = false;
            if(mode == RenderMode::PATHTRACE)
                stopRendering = true;

            // start a thread that stops the program when the user closes stdin
            std::jthread stopThread([&stopRendering]{
                char _[2];
                while(!stopRendering && std::fgets(_, sizeof(_), stdin) != nullptr);

                if(!stopRendering){
                    std::println(stderr, "Will stop rendering after this frame");
                    stopRendering = true;
                }
            });

            // always render once
            renderPathtraceToBuffer();
            writeBufferToFile();

            // keep track of rendered state to be able to average the results (no fancy sampling for incremental yet)
            auto samplesPerPixel = scene.pathtracingSamples.samplesPerPixel;
            auto samplesPerPixelSoFar = samplesPerPixel;
            std::vector<Vec3> previousBuffer = hdrPixelBuffer;

            // TODO this doesnt really work yet
            while (!stopRendering) {
                std::println(stderr, "Frame rendered, {} samples per pixel so far", samplesPerPixelSoFar);

                renderPathtraceToBuffer();
                const auto samplePixelsNow = samplesPerPixel + samplesPerPixelSoFar;
                // average the results
                for(auto [pixel, previousPixel]: std::ranges::views::zip(hdrPixelBuffer, previousBuffer)){
                    pixel = samplesPerPixel*pixel/samplePixelsNow + samplesPerPixelSoFar*previousPixel/samplePixelsNow;
                }
                writeBufferToFile();
                previousBuffer = hdrPixelBuffer;

                samplesPerPixelSoFar = samplePixelsNow;
            } 

            std::println(stderr, "Rendering stopped: Final image has {} samples per pixel", samplesPerPixelSoFar);
        }else{
            // cant try gpu openacc because nvcc doesnt support c++23 :(
            // openacc might not work at all with gcc here, is basically the same time as serial
            //#pragma acc parallel loop
#pragma omp parallel for
            for(uint32_t y = 0; y < scene.camera->heightPixels; y++){
                for(uint32_t x = 0; x < scene.camera->widthPixels; x++){
                    Ray cameraRay = scene.camera->generateRay(Vec2(x, y));

                    auto closestIntersection = traceRayToClosestSceneIntersection(cameraRay);

                    Vec3 pixelColor = scene.backgroundColor;
                    if(closestIntersection.has_value()){
                        if constexpr (mode == RenderMode::BINARY){
                            pixelColor = Vec3(1.0);
                        }else if constexpr (mode == RenderMode::PHONG){
                            pixelColor = shadeBlinnPhong(*closestIntersection);
                        }else{
                            static_assert(false, "Invalid render mode");
                        }
                    }

                    bufferSpecificPixel(Vec2(x, y), linearToneMapping(pixelColor));
                }
            }
        }
        writeBufferToFile();
    }

    void renderDebugBVHToBuffer() {
        // serially, so we dont have to use atomic accesses on the max intensity
        float_T maxIntensity = 0.;
        for(uint32_t y = 0; y < scene.camera->heightPixels; y++){
            for(uint32_t x = 0; x < scene.camera->widthPixels; x++){
                Ray cameraRay = scene.camera->generateRay(Vec2(x, y));

                std::vector<std::pair<BoundingBox, int>> intersected_boxes;
                bvh.recursivelyCollectIntersectedBoxes(cameraRay, intersected_boxes);

                // just write the size to the buffer for now, and keep track of the max
                float_T intensity = intersected_boxes.size();
                maxIntensity = std::max(maxIntensity, intensity);
                bufferSpecificPixel(Vec2(x, y), Vec3(intensity, intensity, intensity));
            }
        }

        // then go through all of them again and normalize them, mark areas close to max intensity as red
        static constexpr float_T redThreshhold = 0.9;
        for(auto& pixel: hdrPixelBuffer){
            float_T intensity = pixel.x;
            if(intensity > redThreshhold * maxIntensity)
                pixel = Vec3(1.0, 0.0, 0.0);
            else
                pixel = pixel / maxIntensity;
        }
    }

    void renderDebugNormalsToBuffer() {
        for(uint32_t y = 0; y < scene.camera->heightPixels; y++){
            for(uint32_t x = 0; x < scene.camera->widthPixels; x++){
                Ray cameraRay = scene.camera->generateRay(Vec2(x, y));

                if(auto closestIntersection = traceRayToClosestSceneIntersection(cameraRay)){
                    auto normalZeroOne = closestIntersection->surfaceNormal * 0.5 + Vec3(0.5);
                    bufferSpecificPixel(Vec2(x, y), normalZeroOne);
                }
            }
        }
    }

    enum ImportanceSamplingTechnique{
        UNIFORM,
        COSINE_WEIGHTED_HEMISPHERE,
    };

    template<ImportanceSamplingTechnique technique>
    Vec3 sampleHemisphere(const Vec3& normal){
        if constexpr (technique == COSINE_WEIGHTED_HEMISPHERE){
            // cosine weighted hemisphere sampling, to eliminate the dot product from the rendering equation
            // basically the approximation of the integral already divides by cos(theta), so the multiplying by theta
            // in the normal rendering equation gets cancelled out.
            // To get the correct value of dividing by the PDF (cos(theta)/pi), we have to multiply by pi again

            // Generate two random numbers for disk sampling
            float_T r = sqrt(randomFloat());
            float_T theta = 2.0 * M_PI * randomFloat();

            // Convert uniform disk samples to hemisphere samples
            float_T x = r * cos(theta);
            float_T y = r * sin(theta);

            // Project up to hemisphere
            float_T z = sqrt(1.0 - x*x - y*y);

            // Create a coordinate system from the normal
            // TODO that z check is strange
            Vec3 up = (std::abs(normal.z) < (1 - 100 * epsilon)) ? Vec3(0, 0, 1) : Vec3(1, 0, 0);
            Vec3 tangent = up.cross(normal).normalized();
            Vec3 bitangent = normal.cross(tangent);

            // Transform the local hemisphere direction to world space
            // TODO hmmm, this will always give positive results though, right?
            return (tangent * x + bitangent * y + normal * z).normalized();
        }else if constexpr (technique == UNIFORM){
            float_T phi = 2.0 * M_PI * randomFloat();
            float_T z = randomFloat();
            float_T r = std::sqrt(1.0 - z*z);

            // Create basis vectors
            Vec3 up = normal;
            Vec3 right(1, 0, 0);
            if (std::abs(up.y) < std::abs(up.x)) {
                right = Vec3(0, 1, 0);
            }

            Vec3 tangent = up.cross(right);
            Vec3 bitangent = up.cross(tangent);

            Vec3 sample = tangent * (r * std::cos(phi)) + 
                bitangent * (r * std::sin(phi)) + 
                up * z;
            assert(sample == sample.normalized() && "Sample must be on the unit hemisphere");

            return sample;
        } else {
            static_assert(false, "Invalid importance sampling technique");
        }
    }

    // Helper function to create orthonormal basis
    std::pair<Vec3, Vec3> createOrthonormalBasis(const Vec3& N) {
        assert(N == N.normalized() && "N must be a normal vector");

        // First, pick a helper vector that's not parallel to N
        Vec3 helper = std::abs(N.z) < 0.999f ? Vec3(0, 0, 1) : Vec3(1, 0, 0);

        // Construct X to be perpendicular to N using the helper
        Vec3 X = N.cross(helper).normalized();

        // Construct Y to be perpendicular to both N and X
        Vec3 Y = N.cross(X);  // Note: no need to normalize since N and X are unit vectors
                                     // and perpendicular to each other

        assert(X == X.normalized() && "X must be normalized");
        assert(Y == Y.normalized() && "Y must be normalized");

        // For debugging, these assertions should now pass
        assert(std::abs(X.dot(Y)) < epsilon && "X and Y must be orthogonal");
        assert(std::abs(N.dot(Y)) < epsilon && "N and Y must be orthogonal");
        assert(std::abs(N.dot(X)) < epsilon && "N and X must be orthogonal");

        return {X, Y};
    }

    template<ImportanceSamplingTechnique samplingTechnique = COSINE_WEIGHTED_HEMISPHERE>
    Vec3 shadePathtraced(const Intersection& intersection, uint32_t bounces = 1){
        if(bounces > scene.nBounces)
            return Vec3(scene.backgroundColor);

        const PrincipledBRDFMaterial& material = intersection.material->assumePrincipledBRDFMaterial();

        // Start with emission
        Vec3 overallColor = material.emissionColor(intersection.textureCoords);

        // Create orthonormal basis for BRDF sampling
        Vec3 N = intersection.surfaceNormal.normalized();
        Vec3 V = (-intersection.incomingRay->direction).normalized();

        // TODO make this into a helper method somewhere, this code is duplicated in so many places
        // Create tangent space basis vectors
        auto [X, Y] = createOrthonormalBasis(N);

        

        // TODO reintroduce comments from previous version

        unsigned actualSamplesTaken = 0;

        // Sample contribution for each hemisphere sample
        Vec3 accumulatedContributions(0.);
        for(unsigned hemisphereSampleNum = 0; 
                hemisphereSampleNum < scene.pathtracingSamples.hemisphereSamplesPerBounce; 
                hemisphereSampleNum++) {

            // Get sample from BRDF
            auto sample = material.sampleBRDF(V, N, X, Y);

            // Skip invalid samples
            if (sample.pdf <= epsilon) {
                continue;
            }

            // in this case, we're going to actually use the sample
            actualSamplesTaken++;

            // Create and trace ray in sampled direction
            Ray incomingRay(
                    intersection.point + sample.direction * (10 * epsilon), 
                    sample.direction
                    );

            Vec3 incomingColor = scene.backgroundColor;
            if(auto incomingIntersection = traceRayToClosestSceneIntersection(incomingRay)) {
                incomingColor = shadePathtraced(*incomingIntersection, bounces + 1);
            }

            // Evaluate BRDF for this direction
            Vec3 brdfValue = material.evaluateBRDF(
                    intersection.textureCoords,
                    V,
                    sample.direction,
                    X, Y, N
                    );

            float_T cosTheta = std::max(0.0f, N.dot(sample.direction));


            // Accumulate contribution
            // Note: cosine term is included in BRDF evaluation
            accumulatedContributions += incomingColor * brdfValue * cosTheta/sample.pdf;
        }

        if(actualSamplesTaken > 0)
            // Average the samples
            overallColor += accumulatedContributions / actualSamplesTaken;

        if(!scene.pointLights.empty()){
            // sample point lights explicitly, because they are infinitessimally small, they can never be hit by a random ray
            // luckily, the pdf of the dirac delta distribution representing these cancels out with the light intensity of the point light itself, so we can simply add it, if the light is not in shadow
            // we could just sample all point lights for every bounce, but thats a bit wasteful again for the later bounces
            // -> randomly pick one, then compensate for that choice by multiplying with the number of point lights
            const auto& light = scene.randomPointLight();

            // optionally sample each light source multiple times
            // not strictly necessary because of monte carlo - we're sampling each pixel multiple times anyway
            // but this gives greater control, although it should be 1 in most cases
            Vec3 accumulatedContributions(0.);

            for(unsigned lightSampleNum = 0; lightSampleNum < scene.pathtracingSamples.pointLightsamplesPerBounce; lightSampleNum++){
                // permute the origin randomly if the light has some amount of softness
                Vec3 intersectionOriginPlusJitter = intersection.point + Vec3(randomFloat() - 0.5, randomFloat() - 0.5, randomFloat() - 0.5) * light.shadowSoftness;

                // TODO hmm, this would be the more accurate version, because we dont want to just sample in an even sphere around the intersection point, just on the surface
                // this does reduce noise, but it introduces strange artifacts for triangle meshes
                //                    Vec3 tangent(0.), bitangent(0.);
                //if (std::abs(intersection.surfaceNormal.x) < std::abs(intersection.surfaceNormal.y)) {
                //    tangent = Vec3(0, intersection.surfaceNormal.z, -intersection.surfaceNormal.y).normalized();
                //} else {
                //    tangent = Vec3(intersection.surfaceNormal.z, 0, -intersection.surfaceNormal.x).normalized();
                //}
                //bitangent = intersection.surfaceNormal.cross(tangent);
                //
                //// Generate random offset in tangent space
                //float r = sqrt(randomFloat()) * light.shadowSoftness; // sqrt for uniform disk sampling
                //float theta = randomFloat() * 2 * M_PI;
                //float x = r * cos(theta);
                //float y = r * sin(theta);
                //
                //// Apply offset in tangent space
                //Vec3 intersectionOriginPlusJitter = intersection.point +
                //(tangent * x + bitangent * y);

                Vec3 L = (light.position - intersectionOriginPlusJitter).normalized();
                Vec3 shadowRayOrigin = intersectionOriginPlusJitter + L * (100 * epsilon);

                Ray shadowRay(shadowRayOrigin, L);

                if(isInShadow(intersection, light, shadowRay))
                    continue;

                // TODO I think this is the wrong way around for point lights

                Vec3 brdf = material.evaluateBRDF(intersection.textureCoords, V, L, X, Y, N);
                // weight the contribution by the angle between the normal and the light ray
                float_T weight = std::max(intersection.surfaceNormal.dot(L), (float_T) 0.);
                accumulatedContributions += brdf * weight * light.intensityPerColor;
            }

            // compensate for only sampling one light
            accumulatedContributions *= scene.pointLights.size();
            // compensate for the number of samples
            accumulatedContributions /= scene.pathtracingSamples.pointLightsamplesPerBounce;

            overallColor += accumulatedContributions;
        }

        return overallColor;
    }

    void renderPathtraceToBuffer(){
        // dynamic thread scheduling improves performance by ~10% on cornell box with 6 bounces (because some rays terminate early by bouncing into nothing)
#pragma omp parallel for collapse(2) schedule(dynamic, 64)
        for(uint32_t y = 0; y < scene.camera->heightPixels; y++){
            for(uint32_t x = 0; x < scene.camera->widthPixels; x++){
                const Vec2 pixelOrigin = Vec2(x, y);
                Vec3 colorSum = Vec3(0.);
                for(uint32_t sample = 0; sample < scene.pathtracingSamples.samplesPerPixel; sample++){
                    // permute ray randomly/evenly for jittered sampling
                    Vec2 permutedPixel = pixelOrigin + Vec2(randomFloat() - 0.5, randomFloat() - 0.5);

                    Ray cameraRay = scene.camera->generateRay(permutedPixel);

                    if(auto intersection = traceRayToClosestSceneIntersection(cameraRay)){
                        colorSum += shadePathtraced(*intersection);
                    }else{
                        // background color
                        colorSum += scene.backgroundColor;
                    }
                }

                bufferSpecificPixel(Vec2(x, y), linearToneMapping(gammaCorrect(colorSum / scene.pathtracingSamples.samplesPerPixel)));
            }
        }
    }

};
